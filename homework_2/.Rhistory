# vector of standard error
SE_jack <- c()
z <- c()
time_jack <- system.time(
for(i in 1:B) {
ind <- sample(1:n, n, replace = TRUE)
score2<- score[ind,]
s_vect[i] <- psi_fun(score2)
# jackknife in bootstrap
psi_j <- c()
for(j in 1:n) psi_j[j] <- psi_fun(score2[-j,])
SE_jack[i] <- pre_fact*sqrt(sum((psi_j-mean(psi_j))^2))
z[i]=(s_vect[i]-psi_obs)/SE_jack[i]
}
)
SE_boot <- sd(s_vect)
# confidence intervals
studentized_ci_jack <- psi_obs - SE_boot*quantile(z,prob=c(0.95,0.05))
attr(studentized_ci_jack,"names") <- NULL
studentized_ci_jack
# time requested to complete the task
time_jack
# number of iterations
C <- 10^4; C_loop <- 10^2
# vector of statistics from resampling
s_vect_2 <- rep(0, C)
# vector of standard error
SE_boot_i <- c()
z2 <- c()
time_dboot <- system.time(
for(i in 1:C) {
ind <- sample(1:n, n, replace = TRUE)
score2<- score[ind,]
s_vect_2[i] <- psi_fun(score2)
# double bootstrap
s_vect_loop <- rep(0, C_loop)
for (j in 1:C_loop) {
ind2 <- sample(1:n, n, replace = TRUE)
score3 <- score2[ind2,]
s_vect_loop[j] <- psi_fun(score3)
}
SE_boot_i[i] <- sd(s_vect_loop)
z2[i]=(s_vect_2[i]-psi_obs)/SE_boot_i[i]
}
)
SE_boot <- sd(s_vect_2)
# confidence intervals
studentized_ci_doubleboot <- psi_obs - SE_boot*quantile(z2,prob=c(0.95,0.05), na.rm = T)
attr(studentized_ci_doubleboot,"names") <- NULL
studentized_ci_doubleboot
# time requested to complete the task
time_dboot
psi_fun_boot <- function(data,indices) {
d <- data[indices,]
eig <- eigen(cor(d), symmetric=TRUE, only.values=TRUE)$values
psi = max(eig)/sum(eig)
}
library(boot)
result_nostud <- boot(score, statistic=psi_fun_boot, R=10^4)
boot.ci(result_nostud,type="all")
library(magrittr)
library(purrr)
psi_fun_stud <- function(data,indices,its) {
d <- data[indices,]
eig <- eigen(cor(d), symmetric=TRUE, only.values=TRUE)$values
v <- boot(d, statistic=psi_fun_boot, R=its)%>%pluck("t")%>%var(na.rm = TRUE)
c(max(eig)/sum(eig),v)
}
result <- boot(score, statistic=psi_fun_stud, its=100, R=1000)
boot.ci(result,type="all")
knitr::opts_chunk$set(echo = TRUE)
score <- read.table("student_score.txt", header = TRUE)
score
cor(score)
# define function to calculate eigenratio of cor(score)
psi_fun <- function(data) {
eig <- eigen(cor(data), symmetric=TRUE, only.values=TRUE)$values
return(max(eig) / sum(eig))
}
# compute observed eigenratio
psi_obs <- psi_fun(score)
# number of iterations
B <- 10^4
n <- nrow(score)
pre_fact <- sqrt((n-1)/n)
# vector of statistics from resampling
s_vect <- rep(0, B)
# vector of standard error
SE_jack <- c()
z <- c()
time_jack <- system.time(
for(i in 1:B) {
ind <- sample(1:n, n, replace = TRUE)
score2<- score[ind,]
s_vect[i] <- psi_fun(score2)
# jackknife in bootstrap
psi_j <- c()
for(j in 1:n) psi_j[j] <- psi_fun(score2[-j,])
SE_jack[i] <- pre_fact*sqrt(sum((psi_j-mean(psi_j))^2))
z[i]=(s_vect[i]-psi_obs)/SE_jack[i]
}
)
SE_boot <- sd(s_vect)
# confidence intervals
studentized_ci_jack <- psi_obs - SE_boot*quantile(z,prob=c(0.95,0.05))
attr(studentized_ci_jack,"names") <- NULL
studentized_ci_jack
# time requested to complete the task
time_jack
# number of iterations
C <- 10^4; C_loop <- 10^2
# vector of statistics from resampling
s_vect_2 <- rep(0, C)
# vector of standard error
SE_boot_i <- c()
z2 <- c()
time_dboot <- system.time(
for(i in 1:C) {
ind <- sample(1:n, n, replace = TRUE)
score2<- score[ind,]
s_vect_2[i] <- psi_fun(score2)
# double bootstrap
s_vect_loop <- rep(0, C_loop)
for (j in 1:C_loop) {
ind2 <- sample(1:n, n, replace = TRUE)
score3 <- score2[ind2,]
s_vect_loop[j] <- psi_fun(score3)
}
SE_boot_i[i] <- sd(s_vect_loop)
z2[i]=(s_vect_2[i]-psi_obs)/SE_boot_i[i]
}
)
SE_boot <- sd(s_vect_2)
# confidence intervals
studentized_ci_doubleboot <- psi_obs - SE_boot*quantile(z2,prob=c(0.95,0.05), na.rm = T)
attr(studentized_ci_doubleboot,"names") <- NULL
studentized_ci_doubleboot
# time requested to complete the task
time_dboot
psi_fun_boot <- function(data,indices) {
d <- data[indices,]
eig <- eigen(cor(d), symmetric=TRUE, only.values=TRUE)$values
psi = max(eig)/sum(eig)
}
library(boot)
result_nostud <- boot(score, statistic=psi_fun_boot, R=10^4)
boot.ci(result_nostud,type="all")
library(magrittr)
library(purrr)
psi_fun_stud <- function(data,indices,its) {
d <- data[indices,]
eig <- eigen(cor(d), symmetric=TRUE, only.values=TRUE)$values
v <- boot(d, statistic=psi_fun_boot, R=its)%>%pluck("t")%>%var(na.rm = TRUE)
c(max(eig)/sum(eig),v)
}
result <- boot(score, statistic=psi_fun_stud, its=100, R=1000)
boot.ci(result,type="all")
log_like_gamma <- function(data,parameter){
sum(dgamma(data, shape = parameter[1], rate = parameter[2], log = TRUE))
}
# create data
n <- 15
y <- rgamma(15, shape = 9, rate = 4)
# create grid for parameters
alpha <- seq(1, 30, length = 100)
beta <- seq(0.5, 15, length = 100)
parameter_values <- expand.grid(alpha, beta)
log_like_values <- apply(parameter_values, 1, log_like_gamma, data = y)
log_like_values <- matrix(log_like_values, nrow = length(alpha), ncol = length(beta), byrow = F)
confidence_levels <- c(0,0.5,0.75,0.9,0.95,0.99)
# create contour plot
contour(alpha, beta, log_like_values-max(log_like_values),
levels = -qchisq(confidence_levels,2)/2, xlab = expression(alpha),
labels = as.character(confidence_levels), ylab = expression(beta))
title('Gamma relative log likelihood')
# computing MLE
alpha_hat <- uniroot(function(x) n*log(n*x/sum(y))-n*digamma(x)+sum(log(y)), c(1e-15,15))$root
#alpha_hat
lambda_hat <- n*alpha_hat/sum(y)
#lambda_hat
c(alpha_hat, lambda_hat)
# computing observed information matrix
j_matrix <- matrix(NA, nrow = 2, ncol = 2)
j_matrix[1,1] <- n*trigamma(alpha_hat)
j_matrix[1,2] <- j_matrix[2,1] <- -n/lambda_hat
j_matrix[2,2] <- n*alpha_hat/(lambda_hat^2)
solve(j_matrix)
MLE_SE <- sqrt(diag(solve(j_matrix)))
MLE_SE
# define log likelihood function
log_lik_weibull <- function( data, param){
-sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}
M <- 10000; mat.ci <- matrix(NA, nrow = M, ncol = 2)
for(i in 1:M){
# generating data for simulation
y_sim <- rweibull(15, shape = 7, scale = 155)
# estimating for i-th simulation
weib.y.mle_sim <- optim(c(1,1), fn=log_lik_weibull, hessian=T, method='L-BFGS-B',
lower=rep(1,2), upper=rep(Inf,2), data=y_sim)
# extracting hessian
h <- weib.y.mle_sim$hessian
# estimation of standard error
mle.se_sim <- sqrt(diag(solve(h)))
# computation of i-th confidence interval
mat.ci[i,] <- weib.y.mle_sim$par[1] + qnorm(0.975)*mle.se_sim[1]*c(-1,1)
}
# empirical coverage of the confidence interval
mean(mat.ci[,1] < 7 & mat.ci[,2] > 7, na.rm = T)
n <- 15
y <- rweibull(n, shape = 7, scale = 155)
log_lik_weibul <- function(data, gamma, beta){
-sum(dweibull(data, shape = gamma, scale = beta, log = TRUE))
}
beta <- seq(110, 200, length.out = 100)
profile_lik <- vector(length=100)
for (i in 1:100) {
profile_lik[i] <- nlm(log_lik_weibul, c(5), data=y, beta=beta[i])$minimum
}
plot(beta, -profile_lik-max(-profile_lik), type="l", ylab="profile likelihood")
conf.level <- 0.95
# confidence interval
for (i in 1:100) {
if ( profile_lik[i] == min(profile_lik)) i_max <- i
}
beta_hat <- beta[i_max]
gamma_hat <- uniroot(function(x) n/x + sum(log(y)) -n*log(beta_hat) - n*sum((y/beta_hat)^x*log(y/beta_hat)), c(1,15))$root
hess <- optimHess(c(gamma_hat,beta_hat),log_lik_weibull,data=y)
J_obs_to_minus1 <- solve(hess)
se_wald_95 <- sqrt(J_obs_to_minus1[2,2])
wald_95 <- beta_hat + c(-1,1)*qnorm(0.975)*se_wald_95
plot(beta, -profile_lik-max(-profile_lik), type="l", ylab="profile likelihood")
segments( wald_95[1], -15, wald_95[2], -15, col="red", lty =1, lwd=2  )
text(150,-14.5,"Wald-type 95% confidence interval",col=2)
segments( wald_95[1],1, wald_95[1],
-15.1, col="red", lty=2)
segments( wald_95[2],1, wald_95[2],
-15.1, col="red", lty=2)
points(wald_95[1], -nlm(log_lik_weibul, c(5), data=y, beta=wald_95[1])$minimum-max(-profile_lik), pch=16, col=2, cex=1.5)
points(wald_95[2], -nlm(log_lik_weibul, c(5), data=y, beta=wald_95[2])$minimum-max(-profile_lik), pch=16, col=2, cex=1.5)
M <- 10000; mat.ci <- matrix(NA, nrow = M, ncol = 2)
for(i in 1:M){
# generating data for simulation
y_sim <- rweibull(15, shape = 7, scale = 155)
# estimating for i-th simulation
weib.y.mle_sim <- optim(c(1,1), fn=log_lik_weibull, hessian=T, method='L-BFGS-B',
lower=rep(1,2), upper=rep(Inf,2), data=y_sim)
# extracting hessian
h <- weib.y.mle_sim$hessian
# estimation of standard error
mle.se_sim <- sqrt(diag(solve(h)))
# computation of i-th confidence interval
mat.ci[i,] <- weib.y.mle_sim$par[2] + qnorm(0.975)*mle.se_sim[2]*c(-1,1)
}
# empirical coverage of confidence interval
mean(mat.ci[,1] < 155 & mat.ci[,2] > 155, na.rm = T)
# The arguments above are needed to avoid showing this cell in the output document
#input values
#true mean
theta_sample <- 2
#likelihood variance
sigma2 <- 2
#sample size
n <- 10
#prior mean
mu <- 7
#prior variance
tau2 <- 2
#generate some data
set.seed(123)
y <- rnorm(n,theta_sample, sqrt(sigma2))
#posterior mean
mu_star <- ((1/tau2)*mu+(n/sigma2)*mean(y))/( (1/tau2)+(n/sigma2))
#posterior standard deviation
sd_star <- sqrt(1/( (1/tau2)+(n/sigma2)))
library(rstan)
#launch Stan model
data<- list(N=n, y=y, sigma =sqrt(sigma2), mu = mu, tau = sqrt(tau2))
fit <- stan(file="normal.stan", data = data, chains = 4, iter=2000)
#extract Stan output
sim <- extract(fit)
install_tinytex()
install.packages("tinytex")
knitr::opts_chunk$set(echo = TRUE)
# define log likelihood function
log_lik_weibull <- function( data, param){
-sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}
for(j in 10){
M <- 10000; mat.ci <- matrix(NA, nrow = M, ncol = 2)
for(i in 1:M){
# generating data for simulation
y_sim <- rweibull(15, shape = 7, scale = 155)
# estimating for i-th simulation
weib.y.mle_sim <- optim(c(1,1), fn=log_lik_weibull, hessian=T, method='L-BFGS-B',
lower=rep(1,2), upper=rep(Inf,2), data=y_sim)
# extracting hessian
h <- weib.y.mle_sim$hessian
# estimation of standard error
mle.se_sim <- sqrt(diag(solve(h)))
# computation of i-th confidence interval
mat.ci[i,] <- weib.y.mle_sim$par[1] + qnorm(0.975)*mle.se_sim[1]*c(-1,1)
}
# empirical coverage of the confidence interval
mean(mat.ci[,1] < 7 & mat.ci[,2] > 7, na.rm = T)
}
# define log likelihood function
log_lik_weibull <- function( data, param){
-sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}
empirical_coverage <- c()
for(j in 10){
M <- 10000; mat.ci <- matrix(NA, nrow = M, ncol = 2)
for(i in 1:M){
# generating data for simulation
y_sim <- rweibull(15, shape = 7, scale = 155)
# estimating for i-th simulation
weib.y.mle_sim <- optim(c(1,1), fn=log_lik_weibull, hessian=T, method='L-BFGS-B',
lower=rep(1,2), upper=rep(Inf,2), data=y_sim)
# extracting hessian
h <- weib.y.mle_sim$hessian
# estimation of standard error
mle.se_sim <- sqrt(diag(solve(h)))
# computation of i-th confidence interval
mat.ci[i,] <- weib.y.mle_sim$par[1] + qnorm(0.975)*mle.se_sim[1]*c(-1,1)
}
# empirical coverage of the confidence interval
empirical_coverage[j] <- mean(mat.ci[,1] < 7 & mat.ci[,2] > 7, na.rm = T)
}
empirical_coverage
# define log likelihood function
log_lik_weibull <- function( data, param){
-sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}
empirical_coverage <- c()
for(j in 1:10){
M <- 10000; mat.ci <- matrix(NA, nrow = M, ncol = 2)
for(i in 1:M){
# generating data for simulation
y_sim <- rweibull(15, shape = 7, scale = 155)
# estimating for i-th simulation
weib.y.mle_sim <- optim(c(1,1), fn=log_lik_weibull, hessian=T, method='L-BFGS-B',
lower=rep(1,2), upper=rep(Inf,2), data=y_sim)
# extracting hessian
h <- weib.y.mle_sim$hessian
# estimation of standard error
mle.se_sim <- sqrt(diag(solve(h)))
# computation of i-th confidence interval
mat.ci[i,] <- weib.y.mle_sim$par[1] + qnorm(0.975)*mle.se_sim[1]*c(-1,1)
}
# empirical coverage of the confidence interval
empirical_coverage[j] <- mean(mat.ci[,1] < 7 & mat.ci[,2] > 7, na.rm = T)
}
empirical_coverage
?qnorm
# Simulates with Rstan the above model
# input values
# true probability
p <- 0.5
# Parameters of the prior distribution
a <- 3
b <- 3
# Sample size
n <- 14
# Generate some data
y <- rbinom(n, 1, p)
# Launch Stan model
library(rstan)
data <- list(N=n, y=y, a=a, b=b)
fit <- stan(file="beta-binomial.stan", data = data, chains = 4, iter=2000)
# data{
#  int<lower=0> N;
#  int y[N];
#  real a;
#  real b;
# }
# parameters{
#  real p;
# }
# model{
#  target += bernoulli_lpmf(y | p);
#  target += beta_lpdf(p | a, b);
# }
# Simulates with Rstan the above model
# input values
# true probability
p <- 0.5
# Parameters of the prior distribution
a <- 3
b <- 3
# Sample size
n <- 14
# Generate some data
y <- rbinom(n, 1, p)
# Launch Stan model
library(rstan)
data <- list(N=n, y=y, a=a, b=b)
fit <- stan(file="beta-binomial.stan", data = data, chains = 4, iter=2000)
sim <- extract(fit)
library(bayesplot)
posterior <- as.array(fit)
# Posterior intervals of the parameter theta
mcmc_intervals(posterior, pars = "p", prob_outer = 0.95) +
ggplot2::labs(
title = expression(paste("Posterior interval of ", theta)),
subtitle = "Bold line: 50% interval. Thin line: 95% interval"
)
# Posterior areas of the parameter theta
mcmc_areas(posterior, pars = "p", prob=0.95) +
ggplot2::labs(
title = expression(paste("Posterior areas of ", theta)),
subtitle = "with median and 95% intervals"
)
# Posterior distribution of both the parameter theta and lp__
mcmc_dens(posterior) +
ggplot2::labs(
title = "Posterior distribution of the parameters"
)
curve(dbeta(x, 3,3), col="blue", lwd=2, ylab = "Beta(3,3)")
a_post <- a + sum(y)
b_post <- b + n - sum(y)
library(MASS)
par(mfrow=c(1,2))
# histogram of the posterior distribution of theta simulated by MCMC
hist.scott(sim$p, prob=TRUE, xlab = expression(theta), main=expression(paste("Histogram of ", theta)))
# true analytical posterior
curve(dbeta(x, a_post, b_post), col="blue", lwd=2, add=T)
# qqplot of the MCMC quantile and analytical quantile
x <- rbeta(100, a_post, b_post)
qqplot(ecdf(sim$p)(x), pbeta(x, a_post, b_post), main=expression(paste("qqplot of ", theta)),
xlab="MCMC quantile", ylab="expected quantile")
v = seq(0, 1, length.out = 100)
lines(v,v, col="blue", lwd=2, )
library(MASS)
par(mfrow=c(1,2))
# histogram of the posterior distribution of theta simulated by MCMC
hist.scott(sim$theta, prob=TRUE, xlab = expression(theta), main=expression(paste("Histogram of ", theta)))
#extract Stan output
sim <- extract(fit)
install.packages("rstanarm")
# The arguments above are needed to avoid showing this cell in the output document
#input values
#true mean
theta_sample <- 2
#likelihood variance
sigma2 <- 2
#sample size
n <- 10
#prior mean
mu <- 7
#prior variance
tau2 <- 2
#generate some data
set.seed(123)
y <- rnorm(n,theta_sample, sqrt(sigma2))
#posterior mean
mu_star <- ((1/tau2)*mu+(n/sigma2)*mean(y))/( (1/tau2)+(n/sigma2))
#posterior standard deviation
sd_star <- sqrt(1/( (1/tau2)+(n/sigma2)))
library(rstan)
#launch Stan model
data<- list(N=n, y=y, sigma =sqrt(sigma2), mu = mu, tau = sqrt(tau2))
fit <- stan(file="normal.stan", data = data, chains = 4, iter=2000)
#extract Stan output
sim <- extract(fit)
library(MASS)
par(mfrow=c(1,2))
# histogram of the posterior distribution of theta simulated by MCMC
hist.scott(sim$theta, prob=TRUE, xlab = expression(theta), main=expression(paste("Histogram of ", theta)))
# true analytical posterior
curve(dnorm(x, mu_star, sd_star), col="blue", lwd=2, add=T)
# qqplot of the MCMC quantile and analytical quantile
x <- rnorm(100, mu_star, sd_star)
qqplot(ecdf(sim$theta)(x), pnorm(x, mu_star, sd_star), main=expression(paste("qqplot of ", theta)),
xlab="MCMC quantile", ylab="expected quantile")
v = seq(0, 1, length.out = 100)
lines(v,v, col="blue", lwd=2, )
log_lik_weibul <- function(data, gamma, beta){
-sum(dweibull(data, shape = gamma, scale = beta, log = TRUE))
}
beta <- seq(110, 200, length.out = 100)
profile_lik <- vector(length=100)
for (i in 1:100) {
profile_lik[i] <- nlm(log_lik_weibul, c(5), data=y, beta=beta[i])$minimum
}
plot(beta, -profile_lik-max(-profile_lik), type="l", ylab="profile likelihood")
library(MASS)
par(mfrow=c(1,2))
x <- seq(1,4,length.out=100)
# Plot of the cdf of the posterior distribution of theta simulated by MCMC
plot(x, ecdf(sim$theta)(x), xlab=expression(theta), main=expression(paste("CDF of",theta)))
# true analytical posterior
curve(pnorm(x, mu_star, sd_star), col="blue", lwd=2, add=T)
# qqplot of the MCMC quantile and analytical quantile
x <- rnorm(100, mu_star, sd_star)
qqplot(ecdf(sim$theta)(x), pnorm(x, mu_star, sd_star), main=expression(paste("qqplot of ", theta)),
xlab="MCMC quantile", ylab="expected quantile")
v = seq(0, 1, length.out = 100)
lines(v,v, col="blue", lwd=2, )
# computing MLE
alpha_hat <- uniroot(function(x) n*log(n*x/sum(y))-n*digamma(x)+sum(log(y)), c(1e-15,15))$root
#alpha_hat
lambda_hat <- n*alpha_hat/sum(y)
#lambda_hat
c(alpha_hat, lambda_hat)
conf.level <- 0.95
# confidence interval
for (i in 1:100) {
if ( profile_lik[i] == min(profile_lik)) i_max <- i
}
beta_hat <- beta[i_max]
gamma_hat <- uniroot(function(x) n/x + sum(log(y)) -n*log(beta_hat) - n*sum((y/beta_hat)^x*log(y/beta_hat)), c(1,15))$root
