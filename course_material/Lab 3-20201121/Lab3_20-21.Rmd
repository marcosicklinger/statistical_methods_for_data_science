---
title: "Laboratory 3"
author: "G Di Credico, L Egidi, N Torelli"
date: "13 November 2020"
output:
  html_document:
    css: style.css
    toc: yes
  pdf_document:
    highlight: tango
    keep_tex: yes
    toc: yes
institute: University of Trieste
subtitle: Statical Methods for Data Science
fontsize: 10pt
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/')
library(MASS)
```
```{r setup, include=FALSE}
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
```

# Weibull model


Let $y=(y_{1},\ldots, y_{n})$ a sample of iid values from a Weibull distribution, $Y \sim \mbox{We}(\gamma, \beta)$, with parameter $\theta=(\gamma, \beta)$ and density function:

$$ f (y; \gamma, \beta)= \frac{\gamma}{\beta}\left(\frac{y}{\beta}\right)^{\gamma-1}e^{-(y/\beta)^{\gamma}}, \ \ \ y, \gamma, \beta >0,$$
where $\gamma$ is the *shape* parameter and $\beta$ is the *scale* parameter.

```{r weibull density}
curve(dweibull(x, shape = 0.5, scale = 1), from = 0, to = 3, ylab = "f(y)", xlab = "y")
curve(dweibull(x, shape = 1, scale = 1), col = 2, add = TRUE)
curve(dweibull(x, shape = 1, scale = 2), col = 3, add = TRUE)
curve(dweibull(x, shape = 1.5, scale = 1), col = 4, add = TRUE)
curve(dweibull(x, shape = 5, scale = 1), col = 5, add = TRUE)
legend("topright", legend=c(expression(gamma == 0.5 *";"~ beta == 1),
                       expression(gamma == 1 *";"~ beta == 1),
                       expression(gamma == 1 *";"~ beta == 2),
                       expression(gamma == 1.5 *";"~ beta == 1),
                       expression(gamma == 5 *";"~ beta == 1)), 
       col=c(1:5), lty=1, cex=0.8, box.lty=0)
```

Then, the likelihood is defined as:

$$L( \gamma, \beta ;y )=\prod_{i=1}^{n}L_{i}=\prod_{i=1}^{n}f(y_i;\gamma, \beta),$$
and the log-likelihood is defined as:

\begin{align*}
l(\gamma, \beta; y)= \log\left( \prod_{i=1}^{n}L_{i}\right)= & \sum_{i=1}^{n} \log f(y_i;\gamma, \beta) \\ 
 = & n(\log(\gamma)-\log(\beta))- (\gamma-1)\sum_{i=1}^{n}\log(y_{i}/\beta)-\sum_{i=1}^{n}(y_{i}/\beta)^{\gamma} \\
= & n\log(\gamma)-n \gamma \log(\beta)+ \gamma\sum_{i=1}^{n}\log(y_{i})-\sum_{i=1}^{n}(y_{i}/\beta)^{\gamma}.
\end{align*}


We may write the log-likelihood function in $\mathsf{R}$:

```{r weibull_log, echo=TRUE}

log_lik_weibull <- function( data, param){
  -sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}

```

## Plotting the log-likelihood

Suppose now to observe $n = 15$ failure times (expressed in days) of a sample of light bulbs: we assume $y_{1}, \ldots, y_{15} \sim \mbox{We}(\gamma, \beta)$, with $\gamma$ and $\beta$ unknown. We need to estimate them via maximum likelihood estimation. Preliminarily, we could inspect the log-likelihood function through **contour** plots.

```{r weibull_log_2, echo=TRUE}
n <- 15
y <- rweibull(n, shape = 7, scale = 155)
# y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146,
#  170.3, 148, 140, 118, 144, 97)
# n <- length(y)

 #define parameters grid
 gamma <- seq(0.1, 15, length=100)
 beta <- seq(100,200, length=100)
 parvalues <- expand.grid(gamma,beta)
 llikvalues <- apply(parvalues, 1, log_lik_weibull, data=y)
 llikvalues <- matrix(-llikvalues, nrow=length(gamma), 
                      ncol=length(beta), byrow=F)
 conf.levels <- c(0,0.5,0.75,0.9,0.95,0.99)
 
 #contour plot
 contour(gamma, beta, llikvalues-max(llikvalues), ##- likelihood-ratio stat/2
 levels=-qchisq(conf.levels, 2)/2,
 xlab=expression(gamma),
 labels=as.character(conf.levels),
 ylab=expression(beta))
 title('Weibull relative log likelihood')
 
 #image
 image(gamma,beta,llikvalues-max(llikvalues),zlim=c(-6,0),
 col=terrain.colors(20),xlab=expression(gamma),
 ylab=expression(beta))
 title('Weibull relative log likelihood')

```


## Parameter estimates


We may compute the maximum likelihood estimates $\hat{\gamma}, \hat{\beta}$ by equating at zero the log-likelihood derivatives:

\begin{align*}
\frac{\partial}{\partial \gamma}l(\gamma, \beta; y)= &  \frac{n}{\gamma}-n \log(\beta)+ \sum_{i=1}^{n}\log(y_{i})-\sum_{i=1}^{n}(y_{i}/\beta)^{\gamma}\log( y_{i}/\beta)  = 0 \\
\frac{\partial}{\partial \beta}l(\gamma, \beta; y)= &  -\frac{n}{\beta} \gamma + \frac{\gamma} {\beta^{\gamma+1}} \sum_{i=1}^{n} y_{i}^{\gamma} = 0
\end{align*}


Solving the second equation we get the constrained estimate $\beta_{\gamma}= (\sum_{i=1}^{n} y^{\gamma}_{i}/n)^{1/\gamma}$. Substituting this in the first equation, we get

$$\frac{n}{\gamma}+\sum_{i=1}^{n} \log (y_{i})-n \frac{\sum_{i} y^{\gamma}_{i}\log(y_{i})}{\sum_{i} y^{\gamma}_{i}} =0$$

The last equation needs to be solved numerically.

```{r weibull_parameters, echo=TRUE}
gammahat <- uniroot(
  function(x) n/x + sum(log(y)) - n*sum(y^x*log(y))/sum(y^x), c(1e-5,15))$root

betahat <- mean(y^gammahat)^(1/gammahat)
weib.y.mle <- c(gammahat, betahat)
#first element is the MLE for the shape gamma, second element the MLE for the scale beta
weib.y.mle
```


Of course, we are interested in assessing the variability and the consistency of our MLE estimators. Let $\theta=(\gamma, \beta)$ denote the bidimensional parameter vector. At this purpose, we may compute the **observed information** matrix $$J(\theta; y)=- \frac{\partial^{2} l( \theta;y)}{ \partial \theta \partial \theta^{T}}.$$ Taking its inverse, evaluated in $(\hat{\gamma}, \hat{\beta})$,  we obtain an estimate for the variance of our estimators. In what follow, we assume to have compute these quantities.


```{r weibull_parameters_2, echo=TRUE}

 #observed information matrix
 jhat <- matrix(NA, nrow=2, ncol=2)
 jhat[1,1] <- n/gammahat^2 + sum((y/betahat)^gammahat* (log(y/betahat))^2)
 jhat[1,2] <- jhat[2,1] <- n/betahat-sum(y^gammahat/betahat^(gammahat+1)*
                                           (gammahat*log(y/betahat)+1))
 jhat[2,2]<- -n*gammahat/betahat^2+gammahat*(gammahat+1)/
   betahat^(gammahat+2)*sum(y^gammahat)
 
 solve(jhat)

 #se of the mle
 mle.se <- sqrt(diag(solve(jhat)))
 mle.se
```


**Exercise 1** 

Compute the MLE and the observed information matrix for a gamma model with parameters shape $\alpha$ and scale $\beta$.


## Numerical optimization

So far, we used $\mathsf{R}$ simply as a pocket calculator, computing MLE and the variance of our estimators *analitically*, and then obtaining the numerical values just plugging into the formulas the inputs. However, remind the MLE for $\gamma$, where the equation:

$$\frac{n}{\gamma}+\sum_{i=1}^{n} \log (y_{i})-n \frac{\sum_{i} y^{\gamma}_{i}\log(y_{i})}{\sum_{i} y^{\gamma}_{i}} =0$$
does not have an analytical solution. Many times we do not have a closed form for MLE estimates and we may need **numerical optimization**. $\mathsf{R}$ provides various functions for performing numerical methods:

- $\mathsf{nlm()}$: minimizes a function using a Newton-type algorithm. It needs a starting value and does not allow constraints on the parameters. It is usually fast and reliable. It  returns the ML estimate $\hat{\theta}$ ($\mathsf{estimate}$), the value of the likelihood $-l(\hat{\theta})$ ($\mathsf{minimum}$) and the hessian ($\mathsf{hessian}$), if $\mathsf{hessian = TRUE}$.

- $\mathsf{optim()}$: minimizes a function using Nelder-Mead, quasi-Newton and conjugate gradients algorithms. It includes an option for box-constrained optimization, and it requires a starting value. It returns  the ML estimate $\hat{\theta}$ ($\mathsf{par}$) and the value of the likelihood $-l(\hat{\theta})$ ($\mathsf{value}$).

- $\mathsf{nlminb()}$: often is more stable, robust and reliable than $\mathsf{optim}$ (in particular with "nasty" functions). It performs only minimization and does not yield numerical derivatives as output. It  returns the ML estimate $\hat{\theta}$ ($\mathsf{par}$) and the value of the likelihood $-l(\hat{\theta})$ ($\mathsf{objective}$).

- $\mathsf{optimize()}$: searches in an interval for a minimum or a maximum (if
$\mathsf{maximum=TRUE}$) of a function. It returns  the ML estimate $\hat{\theta}$ ($\mathsf{minimum}$) and the value of the likelihood $l(\hat{\theta})$ ($\mathsf{objective}$).

**Extra**
```{r weibull_numerical, echo=TRUE}
weib.y.nlm <- nlm(log_lik_weibull,c(0,0),hessian=T,data=y)
```

A warning appears...to avoid numerical warnings, we should work with a parameters' **reparametrization**:

$$ w= w(\theta)=(\log (\gamma), \log(\beta)). $$

Generally, for numerical purposes, it is convenient to reparameterize the model in such a way that the new parameter space is unbounded (in this case, $\theta = \theta(w)= (e^{\gamma}, e^{\beta})$). At such point, the parameters estimates will be expressed in the log-scale, and we need to re-express them in the original scale:

```{r weibull_reparam, echo=TRUE}

#omega <- function(theta) log(theta)
theta <- function(omega) exp(omega)
log_lik_weibull_rep <- function(data, param) log_lik_weibull(data, theta(param))

weib.y.nlm <- nlm(log_lik_weibull_rep, c(0,0) ,hessian=T,data=y)
weib.y.nlm
theta(weib.y.nlm$estimate)
weib.y.mle
```

The $\mathsf{optim()}$ function provides a lot of numerical methods, such us Nelder-Mead, quasi-Newton, conjugate-gradient methods and simulated annealings. As a drawback, the user has to set up very carefully the initial parameters and the adopted method, since the final solution may be quite sensitive to these choices...To compute the observed information, the function $\mathsf{optimHess()}$ computes numerical derivatives of generic functions.


```{r weibull_optim, echo=TRUE}

# gamma = 1, beta = 20 as initial choices with quasi-Newton method
weib.y.optim.qn <- optim(c(1,20), log_lik_weibull_rep, NULL, method =  "BFGS", data=y)$par
theta(weib.y.optim.qn)
# gamma = 1, beta = 6 as initial choices with conjugate-gradient method.
weib.y.optim.cg <- optim(c(1,6), log_lik_weibull_rep, NULL, method =  "CG", data=y)$par
theta(weib.y.optim.cg)
```

```{r weibull_hidden, echo=TRUE}
optimHess(theta(weib.y.nlm$estimate),log_lik_weibull,data=y)
jhat
```


## Profile likelihood

In practical situations, some components of the parameter vector $\theta$ are more important than others; essentially, in such situations it is of interest for us making inference only on those subgroups of parameters. In the Weibull case, we could treat $\gamma$ as the *parameter of interest* and $\beta$ as the *nuisance parameter*. We may then define the profile **log-likelihood**:

$$l_{P}(\gamma) = \underset{\beta}{\max}\ l(\gamma, \beta; y)= l(\gamma, \hat{\beta}_{\gamma}; y),$$
where $\hat{\beta}_{\gamma}$ is the *constrained* MLE for $\beta$, with $\gamma$ fixed. Some issues deserve a quick consideration:

- the profile log-likelihood is simply the log-likelihood for the bidimensional parameter $\theta$, with the nuisance component $\beta$ replaced by $\hat{\beta}_{\gamma}= (\sum_{i=1}^{n} y^{\gamma}_{i}/n)^{1/\gamma}$.

- $l_{P}$ is not a *genuine* likelihood. However, it has some nice features which ease to work with it.


```{r weibull_profile, echo=TRUE}
 weib.y.mle <- optim(c(1,1), fn=log_lik_weibull, hessian=T, method='L-BFGS-B',
                     lower=rep(1e-7,2), upper=rep(Inf,2), data=y)
 gamma <- seq(0.1, 15, length=100)
 beta <- seq(100,200, length=100)
 parvalues <- expand.grid(gamma,beta)
 llikvalues <- apply(parvalues, 1, log_lik_weibull, data=y)
 llikvalues <- matrix(-llikvalues, nrow=length(gamma), ncol=length(beta),
 byrow=F)
 conf.levels <- c(0,0.5,0.75,0.9,0.95,0.99)
 
 #contour plot
 contour(gamma, beta, llikvalues-max(llikvalues),
 levels=-qchisq(conf.levels, 2)/2,
 xlab=expression(gamma),
 labels=as.character(conf.levels),
 ylab=expression(beta))
 title('Weibull profile log-likelihood')

 beta.gamma<- sapply(gamma,function(x) mean(y^x)^(1/x))
 lines(gamma, beta.gamma, lty='dashed',col=2)
 points(weib.y.mle$par[1],weib.y.mle$par[2])
```


In some sense, we *reduced the dimension* of the problem, and we acknowledged that we may work with a one-dimensional likelihood evaluated in the constrained value $\hat{\beta}_{\gamma}$ for the nuisance component. Then, we may now compute some **deviance confidence intervals** with level $1-\alpha$ as:

$$ \{ \gamma: l_{P}(\gamma) \ge l_{P}(\hat{\gamma})-\frac{1}{2}\chi^{2}_{1; 1-\alpha} \},$$
where $\chi^{2}_{1; 1-\alpha}$ is the $1-\alpha$-th quantile of a chi-squared distribution with 1 d.f, the asymptotic distribution for the **profile likelihood-ratio test statistic**:

$$W_{P}(\gamma)=2 \{ l_{P}(\hat{\gamma})-l_{P}(\gamma) \}.$$


```{r weibull_interval, echo=TRUE}
log_lik_weibull_profile  <- function(data, gamma){
  beta.gamma <- mean(data^gamma)^(1/gamma)
 log_lik_weibull( data, c(gamma, beta.gamma) )
}

log_lik_weibull_profile_v <- Vectorize(log_lik_weibull_profile, 'gamma')

plot(function(x) -log_lik_weibull_profile_v(data=y, x)+weib.y.mle$value,
     from=0.1,to=15,xlab=expression(gamma),
     ylab='profile relative log likelihood', ylim=c(-8,0))
 
conf.level <- 0.95
abline(h=-qchisq(conf.level,1)/2,lty='dashed',col=2)
 
lrt.ci1 <- uniroot(function(x) -log_lik_weibull_profile_v(y, x)+
                     weib.y.mle$value+
                     qchisq(conf.level,1)/2, 
                   c(1e-7,weib.y.mle$par[1]))$root
lrt.ci1 <- c(lrt.ci1, uniroot(function(x) -log_lik_weibull_profile_v(y,x) + 
                                weib.y.mle$value + qchisq(conf.level,1)/2,
                              c(weib.y.mle$par[1],15))$root)
segments( lrt.ci1[1],-qchisq(conf.level,1)/2, lrt.ci1[1],
          -log_lik_weibull_profile_v(y, lrt.ci1[1]), col="red", lty=2)
segments( lrt.ci1[2],-qchisq(conf.level,1)/2, lrt.ci1[2],
          -log_lik_weibull_profile_v(y, lrt.ci1[2]), col="red", lty=2)

points(lrt.ci1[1], -qchisq(0.95,1)/2, pch=16, col=2, cex=1.5)
points(lrt.ci1[2], -qchisq(0.95,1)/2, pch=16, col=2, cex=1.5)

segments( lrt.ci1[1], -8.1, lrt.ci1[2], -8.1, col="red", lty =1, lwd=2  )
text(7,-7.5,"95% Deviance CI",col=2)
 



```

**Exercise 2** The **Wald confidence interval**  with level $1-\alpha$ is defined as:

$$ \hat{\gamma} \pm z_{1-\alpha/2}j_{P}(\hat{\gamma})^{-1/2}. $$ Compute the Wald confidence interval of level 0.95, plot the results, and evaluate via simulation the empirical coverage of the confidence interval.


**Exercise 3** Repeat the steps above ---write the profile log-likelihood, plot it and find the deviance confidence intervals--- considering this time $\gamma$ as a nuisance parameter and $\beta$ as the parameter of interest.  



# Bayesian statistics

## Normal case with $\sigma^2$ known

Suppose $y_1, \ldots, y_n \sim \mathcal{N}(\theta, \sigma^{2})$, with $\sigma^{2}$ known. In such a case, the log-likelihood is:

$$ l(\theta; y) = -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(y_i-\theta)^2.$$
If we frame our analysis in **Bayesian** inference, this log-likelihood has not to be maximized over, in order to retrieve its maximum (MLE); rather, the log-likelihood is just a quantity connected to the experiment (observation), framed in a broader context where we still need to incorporate a pre-experimental prior belief about $\theta$: technically, a **prior** distribution. Several prior choices are allowed, and the literature about the prior choice is still growing. 

## Conjugate prior

The simplest way in this case is to assume a normal prior:

$$ \theta \sim \pi(\theta) =\mathcal{N}(\mu, \tau^{2}),$$
with fixed *hyperparameters* $\mu$ and $\tau^2$. In such a case, according to the Bayes theorem our posterior distribution is:

$$\pi(\theta|y) \propto \pi(\theta)L(\theta;y)\propto \mathcal{N}(\mu^{*}, \tau^{*2}),$$

where:


\begin{align}
\begin{split}
\mu^{*}= & \frac{\frac{n}{\sigma^{2}}\bar{y}+\frac{1}{\tau^{2}}\mu }{\frac{n}{\sigma^{2}}+\frac{1}{\tau^2}}\\
\tau^{*2}= & \left(\frac{n}{\sigma^{2}}+\frac{1}{\tau^2}\right)^{-1}
\end{split}
\label{eq:posterior_values}
\end{align}


```{r normal, echo=TRUE}
#input values
#true mean
theta_sample <- 2
#likelihood variance
sigma2 <- 2
#sample size
n <- 10
#prior mean
mu <- 7
#prior variance
tau2 <- 2

#generate some data
set.seed(123)
y <- rnorm(n,theta_sample, sqrt(sigma2))

#posterior mean
mu_star <- ((1/tau2)*mu+(n/sigma2)*mean(y))/( (1/tau2)+(n/sigma2))
#posterior standard deviation
sd_star <- sqrt(1/( (1/tau2)+(n/sigma2)))


curve(dnorm(x, theta_sample, sqrt(sigma2/n)),xlim=c(-4,15), lty=2, lwd=1, col="black", ylim=c(0,1.4), 
ylab="density", xlab=expression(theta))

curve(dnorm(x, mu, sqrt(tau2) ), xlim=c(-4,15), col="red", lty=1,lwd=2,  add =T)
curve(dnorm(x, mu_star, sd_star), 
  xlab=expression(theta), ylab="", col="blue", lwd=2, add=T)  
legend(8.5, 0.7, c("Prior", "Likelihood", "Posterior"), 
  c("red", "black", "blue", "grey" ), lty=c(1,2,1),lwd=c(1,1,2), cex=1)
```

$\mu^{*}$ is a weighted mean of the MLE $\bar{y}$ and the prior mean $\mu$. Hence, by increasing either the prior variance $\tau^2$ or the sample size $n$, our posterior will tend to the likelihood, and the posterior mean will approximately coincide with the MLE $\bar{y}$. 


In such a case, we may obtain a **closed** form for the posterior distribution, *which is still a normal distribution with updates parameters*. This issue is well known as **conjugacy**: the prior and the posterior distributions belong to the same parametric family. However, in the majority of the cases the conjugacy is not guaranteed, since the priors we are going to choose do not belong to the same distribution family  of the posterior.

For all these cases, a closed form is not obtained, and we require **simulation methods**, such as **Markov Chain Monte Carlo** methods. According to these procedures, the posterior distribution is approximately described by a sequence of random draws, a Markov chain.

Several softwares have been developed in these years for these purposes, and this growing *work in progress* made the Bayesian inference a sort of milestone for computational scientists. For illustration purposes, we will use here the $\mathsf{Stan}$ software to compute the posterior distribution via simulation.

```{r normal_stan, echo=TRUE} 

library(rstan)
#launch Stan model
data<- list(N=n, y=y, sigma =sqrt(sigma2), mu = mu, tau = sqrt(tau2))
fit <- stan(file="normal.stan", data = data, chains = 4, iter=2000)
#extract Stan output
sim <- extract(fit)

#draw the true analytical posterior and the Stan simulated posterior

par(mfrow=c(1,2), pty ="m", oma=c(0,0,0,0))
curve(dnorm(x, theta_sample, sqrt(sigma2/n)),xlim=c(-4,15), lty=2, lwd=1, col="black", ylim=c(0,1.2), 
  ylab="density", xlab=expression(theta), cex.lab=2)

curve(dnorm(x, mu, sqrt(tau2) ), xlim=c(-4,15), col="red", lty=1,lwd=2,  add =T)
curve(dnorm(x, mu_star, sd_star), 
  xlab=expression(theta), ylab="", col="blue", lwd=2,
  cex.lab=2, add=T)  

legend(5, 1, c("Prior", "Likelihood", "True Posterior"), 
  c("red", "black", "blue", "blue" ), lty=c(1,2,1),lwd=c(1,1,2), cex=0.8)


curve(dnorm(x, theta_sample, sqrt(sigma2/n)),xlim=c(-4,15), lty=2, lwd=1, col="black", ylim=c(0,1.2), 
  ylab="density", xlab=expression(theta), cex.lab=2)
curve(dnorm(x, mu, sqrt(tau2) ), xlim=c(-4,15), col="red", lty=1,lwd=2,  add =T)
lines(density(sim$theta, adj=2), col ="blue", lwd=2, lty =1)
legend(5, 1, c("Prior", "Likelihood", "Stan Posterior"),
  c("red", "black", "blue", "blue" ), lty=c(1,2,1),lwd=c(1,1,2), cex=0.8)

```

As may be seen, the two posteriors quite coincide. In this simple case, performing MCMC does not provide any benefit over the analytical computations. But if we move away from the _comfortable_ case for the couple prior-likelihood, as for the normal-normal example, MCMC becomes essential.

**Exercise 4** In $\mathsf{sim}$ in the code above, you find the MCMC output which allows to approximate the posterior distribution of our parameter of interest with $S$ draws of $\theta$. Plot the empirical cumulative distribution function and compare it with the theoretical cumulative distribution function of the posterior distribution.

**Exercise 5** Launch the following line of $\mathsf{R}$ code:

```{r bayesplot, echo=TRUE} 
posterior <- as.array(fit)
```

Use now the $\mathsf{bayesplot}$ package. Read the help and produce for this example, using the object ```posterior```, the following plots:

- posterior intervals.
- posterior areas.
- marginal posterior distributions for the parameters.

Quickly comment.

## Non-conjugate priors


Let's change the assumption of the model above, and consider now a uniform prior for $\theta$:

$$ \theta \sim \mathsf{Unif}(-10,10).$$

If we compute the numerator of the Bayes theorem we obtain:


$$ \pi(\theta|y) \propto  L(\theta;y)\pi(\theta)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}\sum_{i}(y_i-\theta)^2}\mbox{I}_{[-10,10]}(\theta),  $$

where $\mbox{I}_{[-10,10]}(\theta)$ is the usual event indicator for the Uniform distribution, equals 1 if $\theta \in[-10,10]$, and 0 otherwise. This product is not proportional to any known distribution, and in such a case we need simulation.



```{r normal_stan_unif, warning=FALSE, message = FALSE, results='hide', echo=TRUE} 

#launch Stan model with the uniform prior in place of normal prior

data2<- list(N=n, y=y, sigma =sqrt(sigma2), a=-10, b=10)
fit2 <- stan(file="normal_uniform.stan", data = data2, chains = 4, iter=2000,
  refresh=-1)

#extract the Stan output
sim2 <- extract(fit2)

#plot the Stan posterior

curve(dnorm(x, theta_sample, sqrt(sigma2/n)),xlim=c(-12,12), lty=2, lwd=1, col="black", ylim=c(0,1.2), 
  ylab="density", xlab=expression(theta))
curve(dunif(x, -10,10  ), xlim=c(-12,12), col="red", lty=1,lwd=2,  add =T)
lines(density(sim2$theta, adj=2), col ="blue", lwd=2, lty =1)
legend(5, 1, c("Prior", "Likelihood", "Stan Posterior"), 
  c("red", "black", "blue", "blue" ), lty=c(1,2,1),lwd=c(1,1,2), cex=0.8)

```

Of course, we may control the variance of the Uniform prior $\mathsf{Unif}(a,b)$, amounting at $\frac{1}{12}(b-a)^{2}$, and consequently the posterior distribution.

**Exercise 6** Suppose you receive $n = 15$ phone calls in a day, and you want to build a model to assess their average length. Your likelihood for each call length is $y_i \sim \mathsf{Exponential}(\lambda)$.  Now, you have to choose the prior $\pi(\lambda)$. Please, tell which of these priors is adequate to describe the problem, and provide a short motivation for each of them:

1. $\pi(\lambda)= \mathsf{Beta}(4,2)$;

2. $\pi(\lambda)= \mathsf{Normal}(1,2)$;

3. $\pi(\lambda)=\mathsf{Gamma}(4,2)$;


Now, compute your posterior as $\pi(\lambda|y)\propto L(\lambda;y)\pi(\lambda)$ for the selected prior. If your first choice was correct, you will be able to compute it analitically.

## MCMC world

Inspecting the posterior distribution via simulation according to some Markov Chains draws sounds like magic. Actually, there are strong theorems and complicate statementes behind this fancy result. In the example above, we retrieved the poterior distribution for $\theta$ via $\mathsf{Stan}$ software, which relies on an extension of MCMC, the so called Hamiltonian Monte Carlo sampling. But how can visualize these values?


```{r traceplot,  warning=FALSE, message = FALSE, results='hide', echo =TRUE} 

data2<- list(N=n, y=y, sigma =sqrt(sigma2), a=-10, b=10)
fit2 <- stan(file="normal_uniform.stan", data = data2, chains = 4, iter=2000,
  refresh=-1)
sim2 <- extract(fit2)

#traceplot
traceplot(fit2, pars ="theta")
theta_est <- mean(sim2$theta)
theta_est


```


In this case, running a bunch of chains until they converge to a **stationary distribution** is the usual way.

We can obtain also some MCMC areas:


```{r areas, echo =TRUE} 
library("bayesplot")
library("rstanarm")
library("ggplot2")



#MCMC areas
posterior <- as.matrix(fit2)

plot_title <- ggtitle("Posterior distributions",
                      "with medians and 80% intervals")

mcmc_areas(posterior, 
           pars = "theta", 
           prob = 0.8) + plot_title


```


If the variance $\sigma^2$ of our assumed model above is not known in advance, our model above is **biparametric**. Let's then extend the model above, and consider $\sigma^{2}$ as unknown:

\begin{align*}
y_{1},\ldots,y_{n} & \sim  \mathcal{N}(\theta, \sigma^2)\\
\theta &\sim  \mathsf{Unif}(-10,10)\\
{\sigma} &\sim \mathsf{Cauchy^{+}}(0, 2.5),
\end{align*}

where $\mathsf{Cauchy^{+}}(0, 2.5)$ denotes an Half-Cauchy with location 0 and scale 2.5.

```{r bipar, warning=FALSE, message = FALSE, results='hide', echo=TRUE} 

#launch biparametric Stan model

data3<- list(N=n, y=y, a=-10, b=10)
fit3 <- stan(file="biparametric.stan", data = data3, chains = 4, iter=2000,
  refresh=-1)

#extract stan output for biparametric model

sim3 <- extract(fit3)
posterior_biv <- as.matrix(fit3)

theta_est <- mean(sim3$theta)
sigma_est <- mean(sim3$sigma)
c(theta_est, sigma_est)
traceplot(fit3, pars=c("theta", "sigma"))

plot_title <- ggtitle("Posterior distributions",
                      "with medians and 80% intervals")

mcmc_areas(posterior_biv, 
           pars = c("theta","sigma"), 
           prob = 0.8) + plot_title



```


**Exercise 7** Go to this link: [rstan](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started), and follow the instructions to download and install the $\mathsf{rstan}$ library. Once you did it succesfully, open the file model called $\mathsf{biparametric.stan}$, and replace the line:

$$\mathsf{ target+=cauchy\_lpdf(sigma| 0, 2.5);} $$

with the following one:

$$\mathsf{ target+=uniform\_lpdf(sigma| 0.1, 10);} $$

Which prior are you now assuming for your parameter $\sigma$? Reproduce the same plots as above and briefly comment.


**Extra**
# Model comparisons


In Bayesian inference we can still perform some tests in order to verify some assumptions. Rather than verifying  a particular value for a parameter, we are here more interested in assessing some model comparisons in term of prior choices. Suppose you want to test the hypothesis 

$$H_0: \theta \in \Theta_0; H_1: \theta \in \Theta_1,$$

where $\Theta_0$ and $\Theta_1$ form a partition of the parameter space.

The beliefs about the two hypotheses are summarized by the posterior odds ratio: 

$$\frac{p_0}{p_1}=\frac{P(\theta\in \Theta_0|y)}{P(\theta\in \Theta_1|y)}=\frac{\int_{\Theta_0}\pi(\theta|y)d\theta}{\int_{\Theta_1}\pi(\theta|y)d\theta}$$

A measure of the evidence provided by the data in support of $H_0$ over $H_1$ is the **Bayes factor**:

$$BF_{01}= \frac{\mbox{posterior odds}}{\mbox{prior odds}}=\frac{p_0/p_1}{\pi_0/\pi_1}=\frac{m_{0}(y)}{m_1(y)},$$

where $\pi_0$ and $\pi_1$ are respectively $\int_{\Theta_0}\pi(\theta)d\theta$ and $\int_{\Theta_1}\pi(\theta)d\theta$ the probability of the two hypotheses prior observing the data. $m_0(y)$ and $m_1(y)$, better known as *marginal likelihoods*, are $\int_{\Theta_0}L(\theta_0;y)\pi(\theta_0)d\theta_0$ and $\int_{\Theta_1}L(\theta_1;y)\pi(\theta_1)d\theta_1$ respectively.

## Models for soccer goals

Suppose we are interested in assessing the average number of goals scored by a given team in Major league Soccer, and denote the goals for $n$ games as $y_1,\ldots,y_n$. Since goals are relatively rare events, we assume that:

$$y_i \sim \mathsf{Poisson}(\lambda), \ i=1,\ldots,n,$$
thus, the likelihood is: 

$$L(\lambda; y)=\prod_{i=1}^{n} e^{-\lambda}\frac{\lambda^{y_{i}}}{y_i!}.$$
Now, we have to elicit a prior for the parameter $\lambda$, and several choices are plausible:

- **Prior 1**: $\lambda \sim \mathsf{Gamma}(4.57, 1.43)$, meaning that the average is $\alpha/\beta= 3.195804$ and the quartiles for $\lambda$ are given by 2.10 and 4.04.

- **Prior 2**: $\log(\lambda) \sim \mathcal{N}(1, .5^2)$.  The quartiles for this prior for $\log(\lambda)$ are 0.66 and 1.34, which translates to prior quartiles for $\lambda$ of 1.94 and 3.81.

- **Prior 3**: $\log(\lambda) \sim \mathcal{N}(2, .5^2)$. The quartiles for $\log(\lambda)$ are  1.66 and 2.33, which translates to prior quartiles for $\lambda$ of 5.25 and 10.27.

- **Prior 4**: $\log(\lambda) \sim \mathcal{N}(1, 2^2)$. The quartiles for $\log(\lambda)$ are -0.34  and 2.34, which translates to prior quartiles for $\lambda$ of 0.71 and 10.38.


The dataset $\mathsf{soccergoals}$ in the $\mathsf{LearnBayes}$ package contains the number of goals across the 35 matches of the 2006 season for a given team. Let's write the prior functions and plot the four priors and the likelihood for $\theta=\log(\lambda)$.

```{r testing, warning=FALSE, message = FALSE, results='hide', echo=TRUE} 


library(LearnBayes)
data(soccergoals)

y <- soccergoals$goals

#write the likelihood function via the gamma distribution


lik_pois<- function(data, theta){
  n <- length(data)
  lambda <- exp(theta)
  dgamma(lambda, shape =sum(data)+1, scale=1/n)
}

 prior_gamma <- function(par, theta){
  lambda <- exp(theta)
  dgamma(lambda, par[1], rate=par[2])*lambda  
}

 prior_norm <- function(npar, theta){
 lambda=exp(theta)  
 (dnorm(theta, npar[1], npar[2]))
  
}

lik_pois_v <- Vectorize(lik_pois, "theta")
prior_gamma_v <- Vectorize(prior_gamma, "theta")
prior_norm_v <- Vectorize(prior_norm, "theta")


#likelihood
 curve(lik_pois_v(theta=x, data=y), xlim=c(-1,4), xlab=expression(theta), ylab = "density", lwd =2 )
#prior 1
 curve(prior_gamma_v(theta=x, par=c(4.57, 1.43)), lty =2, col="red", add = TRUE, lwd =2)
#prior 2 
 curve(prior_norm_v(theta=x, npar=c(1, .5)), lty =3, col="blue", add =TRUE, lwd=2)
#prior 3 
 curve(prior_norm_v(theta=x, npar=c(2, .5)), lty =4, col="green", add =TRUE, lwd =2)
#prior 4 
  curve(prior_norm_v(theta=x, npar=c(1, 2)), lty =5, col="violet", add =TRUE, lwd =2)
  legend(2.6, 1.8, c("Lik.", "Ga(4.57,1.43)", "N(1, 0.25)", "N(2,0.25)","N(1, 4)" ),
  lty=c(1,2,3,4,5), col=c("black", "red", "blue", "green", "violet"),lwd=2, cex=0.9)
```

Let's quickly comment. Priors 1 and 2 almost coincide in scale and location, and they propose a mean---in log scale---around 1, which is slightly greater than the likelihood mean, $\log(\bar{\lambda})=0.49$. Prior 3 is quite extreme, and results to be in conflict with the likelihood. Prior 4 is very flat and provides no much information about the parameter $\theta$. 


Now we have to compute the log-posteriors, using the reparametrization $\theta= \log(\lambda)$ and calling the previous functions:

$$\log(\pi(\theta|y))\propto l(\theta;y)+log(\pi(\theta)).$$


```{r testing_2, warning=FALSE, message = FALSE, results='hide', echo=TRUE} 

logpoissongamma <- function(theta, datapar){
   data <- datapar$data
   par <- datapar$par
   lambda <- exp(theta)
   log_lik <- log(lik_pois(data, theta))
   log_prior <- log(prior_gamma(par, theta))
   return(log_lik+log_prior)
}

logpoissongamma.v <- Vectorize( logpoissongamma, "theta")


logpoissonnormal <- function( theta, datapar){
 data <- datapar$data
 npar <- datapar$par
 lambda <- exp(theta)
 log_lik <- log(lik_pois(data, theta))
 log_prior <- log(prior_norm(npar, theta))
  return(log_lik+log_prior)
}  
logpoissonnormal.v <- Vectorize( logpoissonnormal, "theta")

#log-likelihood
curve(log(lik_pois(y, theta=x)), xlim=c(-1,4),ylim=c(-20,2), lty =1,
   ylab="log-posteriors", xlab=expression(theta))
#log posterior 1
curve(logpoissongamma.v(theta=x, list(data=y, par=c(4.57, 1.43))), col="red", xlim=c(-1,4),ylim=c(-20,2), lty =1, add =TRUE)
#log posterior 2
 curve(logpoissonnormal.v( theta=x, datapar <- list(data=y, par=c(1, .5))), lty =1, col="blue",  add =TRUE)
#log posterior 3
 curve(logpoissonnormal.v( theta=x, datapar <- list(data=y, par=c(2, .5))), lty =1, col="green", add =TRUE, lwd =2)
#log posterior 4
  curve(logpoissonnormal.v( theta=x, list(data=y, par=c(1, 2))), lty =1, col="violet", add =TRUE, lwd =2)
 legend(2.6, 1.3, c( "loglik", "lpost 1", "lpost 2", "lpost 3", "lpost 4" ),
  lty=1, col=c("black", "red", "blue", "green", "violet"),lwd=2, cex=0.9)
```
Now we have to compute the Bayes factors. The Bayes factor in support of Prior 1 over Prior 2 is:

$$ BF_{12}=\frac{m_1(y)}{m_2(y)}=\frac{\int_{\Theta_1} L(\theta_1;y)\pi(\theta_1)d\theta_1}{\int_{\Theta_2} L(\theta_2;y)\pi(\theta_2)d\theta_2}.$$

We use the function $\mathsf{laplace()}$ contained in the $\mathsf{LearnBayes}$ package for computing the posterior modes, the posterior standard deviations and the log-marginal likelihoods, $\log(m(y))$.

```{r testing_3, warning=FALSE, message = FALSE, echo=TRUE} 
datapar <- list(data=y, par=c(4.57, 1.43))
fit1 <- laplace(logpoissongamma, .5, datapar)
datapar <- list(data=y, par=c(1, .5))
fit2 <- laplace(logpoissonnormal, .5, datapar)
datapar <- list(data=y, par=c(2, .5))
fit3 <- laplace(logpoissonnormal, .5, datapar)
datapar <- list(data=y, par=c(1, 2))
fit4 <- laplace(logpoissonnormal, .5, datapar)

postmode <- c(fit1$mode, fit2$mode, fit3$mode, fit4$mode )
postsds <- sqrt(c(fit1$var, fit2$var, fit3$var, fit4$var))
logmarg <- c(fit1$int, fit2$int, fit3$int, fit4$int)
cbind(postmode, postsds, logmarg)
```


Now we may compute the Bayes factors, for instance:

$$BF_{21}=m_{2}(y)/m_{1}(y)=\exp\{-1.255171+1.502877 \}= 1.281,$$
which means that Prior 2 is 1.28 times more preferable than Prior 1. This makes sense: if we look at the first plot, Prior 2 was slightly closer to the likelihood.


```{r testing_4, warning=FALSE, message = FALSE, echo=TRUE} 
BF_matrix <- matrix(1, 4,4)
for (i in 1:3){
  for (j in 2:4){
   BF_matrix[i,j]<- exp(logmarg[i]-logmarg[j])
   BF_matrix[j,i]=(1/BF_matrix[i,j]) 
  }
}

round_bf <- round(BF_matrix,3)
round_bf


```

Every prior is favored over Prior 3, wich is the prior with the greatest likelihood conflict. Prior 2 is always favored over the other priors. Generally, the marginal probability for a prior decreases as the prior density becomes more diffuse.



**Exercise 8** Simulate a sample of size $n=14$ from a Bernoulli distribution parameter $p=0.5$.

- Looking at the $\mathsf{Stan}$ code for the other models, write a short $\mathsf{Stan}$ Beta-Binomial model, where $p$ has a $\mathsf{Beta}(a,b)$ prior with $a=3$, $b=3$.

- extract the posterior distribution with the function $\mathsf{extract()}$;

- produce some plots with the $\mathsf{bayesplot}$ package and comment.

- compute analitically the posterior distribution and compare it with the $\mathsf{Stan}$ distribution.