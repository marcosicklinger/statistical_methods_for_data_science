---
title: "Laboratory 5"
author: "G Di Credico, L Egidi, N Torelli"
date: "Fall 2020"
output:
  html_document:
    toc: yes
  beamer_presentation:
    highlight: tango
  include: null
  ioslides_presentation:
    highlight: tango
  pdf_document:
    highlight: tango
    keep_tex: yes
    toc: yes
  slide_level: 2
  slidy_presentation:
    fig.height: 3
    fig.width: 4
    highlight: tango
header-includes:
- \usepackage{color}
- \usepackage{bm}
- \definecolor{Purple}{HTML}{911146}
- \definecolor{Orange}{HTML}{CF4A30}
- \setbeamercolor{alerted text}{fg=Orange}
- \setbeamercolor{frametitle}{bg=Purple}
graphics: yes
subtitle: Simulation, examples and exercises
fontsize: 10pt
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/',options(width=400))
library(MASS)
```
```{r setup, include=FALSE}
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
```


# Generalized Linear Models

Many times, linearity between a dependent variable and a set of predictors is not a suited assumption. Normal linear models rely on a *normality assumption*, and this may be a limitation when
the nature of the dependent variable is discrete and its range spans from 0 to $+\infty$. In normal linear models, the main assumption is:

$$ E[y_i]=\beta_0+\sum_{j=1}^{p}\beta_jx_{ij}.$$


 Extending the linear framework allows for the following transformation involving a twice differentiable **link function** g such that:
 
 $$g(E[y_i])=\beta_0+\sum_{j=1}^{p}\beta_jx_{ij}.$$
 
## Logistic regression 
 The following example is in the book: *Data analysis using regression and multilevel/ hierarchical models*, Gelman, A., \& Hill, J. (2007), Cambridge University Press. 
 
It is known that water in some wells in Bangladesh and other South Asian countries might be contaminated with natural arsenic. The effect of the arsenic ingestion on health is a cumulative poisoning, and the risk of cancer and other diseases is estimated to be proportional to the dose intake. 

A research team from the United States and Bangladesh examined the water of several wells in Araihazar district of Bangladeshin and  they measured the arsenic level classified the wells as *safe* if the arsenic level was below the 0.5 in units of hundreds of micrograms per litre, or *unsafe* if it was above 0.5. All the people using unsafe wells have been encouraged to switch to a nearby well.
Indeed the contamination does not depend on the proximity and close wells can have very different levels of arsenic. 

The dataset in the file `Wells.csv` contains information on whether or not 3020 households in Bangladesh changed the wells that they were using after few years from the arsenic investigation.

  - **switch:** whether or not the household switched to another well from an unsafe well: no (0) or yes (1);
  - **arsenic:** the level of arsenic contamination in the household's original well, in hundreds of micrograms per litre; all are above 0.5, which was the level identified as “safe”;
  - **distance:** in meters to the closest known safe well;
  - **education:** in years of the head of the household;
  - **association:** whether or not any members of the household participated in any community organizations: no (0) or yes (1).
  

```{r load data}
data <- read.csv2("Wells.csv", header=TRUE)
str(data)
summary(data)
```

Using a logistic regression, we analyse which are the main factors of well switching among the users of unsafe wells. 
```{r explorative plots, echo=FALSE}
par(mfrow=c(1,2))
boxplot(data$dist ~ data$switch, horizontal = TRUE, xlab="Distance", ylab="Switch")
boxplot(data$arsenic ~ data$switch, horizontal = TRUE, xlab = "Arsenic", ylab="Switch")
par(mfrow=c(1,1))
barplot(prop.table(table(data$switch, data$educ), margin = 1),
        beside=TRUE, legend.text = c("no switch", "switch"), xlab="Education")
```

### One predictor

We start the analysis modelling the effect of the variable `distance` on the variable `switch`. Since the outcome variable is a binary variable, the model chosen is the logistic regression model. 
 $$\begin{align*}
 Pr(y_i=1)&=p_i\\
 \text{logit}(p_i)&=X_i\beta,
\end{align*}$$
where $\text{logit}(x)=\log(x/(1-x))$ is a function that maps the range $(0,1)$ to the range $(-\infty,\infty)$. 

In R:
```{r logistic one predictor}
## Logistic regression with one predictor

fit.1 <- glm(switch ~ dist, family=binomial(link="logit"), data=data)
summary(fit.1)
```
The intercept can only be interpreted assuming zero values for the other predictors. When zero is not interesting or not even in the model, the intercept must be evaluated at some other point. Here, when `dist`$=0$:
```{r logistic one coeff intercept}
invlogit <- function (x) {1/(1+exp(-x))}
invlogit(fit.1$coefficients%*%c(1,0))
```
Thus the probability of switching well for a family who lives close to a safe well is  about $65\%$.
 
There are two ways of interpreting the coefficients for the predictor `dist`:

  - the coefficient for `dist` can be interpreted as a negative difference of $−0.0062$ in the logit probability of switching well when the distance is increased by one;

Or we can evaluate the difference of the logistic regression function to adding 1 to the predictor `dist`. This difference corresponds to a difference on the probability scale but requires the choice of two specific values of the predictor. The mean of the predictor is a good starting point since it corresponds to the steepest point of the inverse logit function. 
  
Thus,

 - adding 1 to`dist` —that is, adding 1 meters to the distance to the nearest safe well— corresponds to a negative difference in the probability of switching of about $0.15\%$ on average.
  
```{r logistic coeff interpretation}
# difference on the probability to switch well due to an increase of one unit on the predictor from the mean
invlogit(fit.1$coefficients%*%c(1,mean(data$dist)+1))-
 invlogit(fit.1$coefficients%*%c(1,mean(data$dist)))
```

Notice that coefficients here does not have a linear effect on the probability that the outcome is 1 because of the nonlinearity of the model. The curve of the logistic function requires us to choose where to evaluate changes, if we want to interpret on the probability scale.

```{r logistic coeff interpretation2}
# difference on the probability to switch well due to an increase of one unit on the predictor from the 99 percentile
invlogit(fit.1$coefficients%*%c(1,quantile(data$dist,.99)+1))-
  invlogit(fit.1$coefficients%*%c(1,quantile(data$dist,.99)))
```

The effect of one unit increase of the variable `dist` on the inverse logit seems low, but this is misleading since distance is measured in meters, so this coefficient corresponds to the difference between, say, a house that is 90 meters away from the nearest safe well and a house that is 91 meters away. In order to improve interpretability of the results we rescale distance in 100-meter units.

```{r logistic one predictor transf}
## Repeat the regression above with distance in 100-meter units
data$dist100 <- data$dist/100
fit.2 <- glm (switch ~ dist100, family=binomial(link="logit"), data=data)
summary(fit.2)

# difference on the probability scale due to an increase of 1 unit on the predictor (dist100) from the mean
invlogit(fit.2$coefficients%*%c(1,mean(data$dist100)+1))-
  invlogit(fit.2$coefficients%*%c(1,mean(data$dist100)))

#notice that
# difference on the probability scale due to an increase of 100 unit on the predictor from the mean (fit.1)
invlogit(fit.1$coefficients%*%c(1,mean(data$dist)+100))-
  invlogit(fit.1$coefficients%*%c(1,mean(data$dist)))

```
The following plot shows the fitted model with one predictor

```{r logistic one predictor graph, echo=FALSE}
## Graphing the fitted model with one predictor

jitter.binary <- function(a, jitt=.05){
  ifelse (a==0, runif (length(a), 0, jitt), runif (length(a), 1-jitt, 1))
}

switch.jitter <- jitter.binary(data$switch)

plot(data$dist, switch.jitter, xlab="Distance (in meters) to nearest safe well", 
     ylab="Pr (switching)", type="n", xaxs="i", yaxs="i", mgp=c(2,.5,0))
curve (invlogit(coef(fit.1)[1]+coef(fit.1)[2]*x), lwd=1, add=TRUE)
points (data$dist, jitter.binary(data$switch), pch=20, cex=.1)
```

The probability of switching well is about $60\%$ for those who live close to a safe well, declining to about $20\%$ when the distance from a safe well increases up to $300$ meters from their home. This seems reasonable: the probability of switching is higher for people who live closer to a safe well (Gelman-Hill, 2007).


### Adding predictors
We start adding the arsenic level. We expect that the higher the arsenic concentration the higher the probability of switching well, that is, translated in model terms, a positive sign for the arsenic coefficient. 

```{r logistic arsenic, echo=FALSE}
## Histogram on arsenic levels 

hist (data$arsenic, breaks=seq(0,.25+max(data$arsenic[!is.na(data$arsenic)]),.25), 
      freq=TRUE, xlab="Arsenic concentration in well water", 
      ylab="", main="", mgp=c(2,.5,0))

## Logistic regression with second input variable

fit.3 <- glm(switch ~dist100 + arsenic, family=binomial(link="logit"), data=data)
summary(fit.3)
```
Numerical interpretation of one predictor effect here needs to be done choosing a value for the other predictor. As an example: given two wells with the same arsenic level (notice that it cannot be 0!), the *logit probability* of switching decreases of $-.9$ every 100 meters in distance to the nearest safe well. 

Equivalently: for two equally distant nearest safe wells, a difference of 1 in arsenic concentration corresponds to a 0.46 positive difference in the *logit probability* of switching. 

The following plots show the fitted model with two predictors
```{r logistic arsenic graph, echo=FALSE}
## Graphing the fitted model with two predictors

plot(data$dist, switch.jitter, xlim=c(0,max(data$dist)), 
     xlab="Distance (in meters) to nearest safe well", ylab="Pr (switching)", 
     type="n", xaxs="i", yaxs="i", mgp=c(2,.5,0))
curve(invlogit(cbind(1, x/100, .5) %*% coef(fit.3)), lwd=.5, add=TRUE)
curve(invlogit(cbind(1, x/100, 1.0) %*% coef(fit.3)), lwd=.5, add=TRUE)
points(data$dist, jitter.binary(data$switch), pch=20, cex=.1)
text (50, .27, "if As = 0.5", adj=0, cex=.8)
text (75, .50, "if As = 1.0", adj=0, cex=.8)

plot(data$arsenic, switch.jitter, xlim=c(0,max(data$arsenic)), 
     xlab="Arsenic concentration in well water", ylab="Pr (switching)", 
     type="n", xaxs="i", yaxs="i", mgp=c(2,.5,0))
curve(invlogit(cbind(1, 0, x) %*% coef(fit.3)), lwd=.5, add=TRUE)
curve(invlogit(cbind(1, 0.5, x) %*% coef(fit.3)), lwd=.5, add=TRUE)
points(data$arsenic, jitter.binary(data$switch), pch=20, cex=.1)
text(1.5, .78, "if dist = 0", adj=0, cex=.8)
text(2.2, .6, "if dist = 50", adj=0, cex=.8)

```
In general terms, the fitted model can be summarized as: switching is easier if there is a nearby safe well, and if a household’s existing well has a high arsenic level, there should be more motivation to switch.

**Extra** try to interpret the change occurred in the estimated coefficient for the predictor `dist100` in model fit.1 ($\beta^1_{\texttt{dist}}=-0.62$) versus model fit.2 ($\beta^2_{\texttt{dist}}=-0.9$).


We now want to verify if there is a statistically significant effect of the interaction between the distance and the arsenic concentration on the probability of switching well.

```{r logistic with interaction}
# Logistic regression with interactions

fit.4 <- glm (switch ~ dist100 + arsenic + dist100:arsenic, #switch ~ dist100*arsenic equivalently
  family=binomial(link="logit"), data=data)
fit.4 <- glm (switch ~ dist100*arsenic, #switch ~ dist100*arsenic equivalently
  family=binomial(link="logit"), data=data)
summary(fit.4)
```

As said before, the interpretation of the coefficients when arsenic is equal to 0 does not make sense, since the minimum level reached is 0.5. We fix the predictors to their mean and so

  - the probability of switching well for an average distance and for an average arsenic concentration is about 0.59.
```{r}
# constant term
invlogit(c(1,mean(data$dist100),mean(data$arsenic),
           mean(data$dist100)*mean(data$arsenic))%*%coefficients(fit.4))
```

  - the probability of switching well needs to consider also the interaction term, thus: at the mean level of arsenic in the data, each 100 meters of distance corresponds to an approximate 22% negative difference in probability of switching. 
```{r}
# an increse of 100 meters in the distance, helding arsenic at its mean
invlogit(c(1,mean(data$dist100)+1,mean(data$arsenic),
           (mean(data$dist100)+1)*mean(data$arsenic))%*%coefficients(fit.4))-
invlogit(c(1,mean(data$dist100),mean(data$arsenic),
           mean(data$dist100)*mean(data$arsenic))%*%coefficients(fit.4))
```
  
  - while at the mean level of distance in the data, each additional unit of arsenic corresponds to an approximate 11% positive difference in probability of switching.
```{r}
# an increse of 1 in the arsenic, helding distance at its mean
invlogit(c(1,mean(data$dist100),mean(data$arsenic)+1,
           (mean(data$dist100))*(mean(data$arsenic)+1))%*%coefficients(fit.4))-
invlogit(c(1,mean(data$dist100),mean(data$arsenic),
           mean(data$dist100)*mean(data$arsenic))%*%coefficients(fit.4))
```

 - the interaction term can be seen as the effect added to the coefficient for distance for each additional unit of arsenic: at the average level of arsenic, the coefficient for distance is −0.88.  Vice versa, at the average level of distance, the coefficient for arsenic is 0.47. So, the importance of distance as a predictor increases for households with higher existing arsenic levels and the importance of arsenic decreases for households that are farther from existing safe wells.
 
```{r logistic with interaction coeff}
coef(fit.4)[2]+coef(fit.4)[4]*mean(data$arsenic)
coef(fit.4)[3]+coef(fit.4)[4]*mean(data$dist100)
```

Interpreting the coefficients is easier when variables are standardized. Coefficients of predictors are now interpretable as the effect of the predictor on the logit probability when the others are at their mean. 

```{r logistic with interaction center}
## Centering the input variables

data$c.dist100 <- scale(data$dist100, scale = FALSE)
data$c.arsenic <- scale(data$arsenic, scale = FALSE)

## Refitting the model with centered inputs

fit.5 <- glm (switch ~ c.dist100 + c.arsenic + c.dist100:c.arsenic,
  family=binomial(link="logit"), data=data)
summary(fit.5)
```

The same graphs as before can be used to plot the models with the interaction term. This graph shows evidence that the differences in switching associated with differences in arsenic level are large if you are close to a safe well, but with a diminishing effect if you are far from any safe well. Comparing this plot with the one related to the model without interaction highlights that the main difference among the two curves is found in the area with few data points.

```{r logistic with interaction graph, echo=FALSE}
## Graphing the model with interactions

plot(data$dist, switch.jitter, xlim=c(0,max(data$dist)), xlab="Distance (in meters) to nearest safe well", 
   ylab="Pr (switching)", type="n", xaxs="i", yaxs="i", mgp=c(2,.5,0))
curve (invlogit(cbind (1, x/100, .5, .5*x/100) %*% coef(fit.4)), lwd=.5, add=TRUE)
curve (invlogit(cbind (1, x/100, 1.0, 1.0*x/100) %*% coef(fit.4)), lwd=.5, add=TRUE)
points (data$dist, jitter.binary(data$switch), pch=20, cex=.1)
text (50, .37, "if As = 0.5", adj=0, cex=.8)
text (75, .50, "if As = 1.0", adj=0, cex=.8)

plot(data$arsenic, switch.jitter, xlim=c(0,max(data$arsenic)), xlab="Arsenic concentration in well water",
   ylab="Pr (switching)", type="n", xaxs="i", yaxs="i", mgp=c(2,.5,0))
curve (invlogit(cbind (1, 0, x, 0*x) %*% coef(fit.4)), lwd=.5, add=TRUE)
curve (invlogit(cbind (1, 0.5, x, 0.5*x) %*% coef(fit.4)), lwd=.5, add=TRUE)
points (data$arsenic, jitter.binary(data$switch), pch=20, cex=.1)
text (1.5, .8, "if dist = 0", adj=0, cex=.8)
text (2.2, .6, "if dist = 50", adj=0, cex=.8)
```

### Adding social predictors

In order to improve interpretability of the results, we divide the education variable by 4 and we centre the predictor. 

**Extra** try to interpret the coefficients of the new model.
```{r logistic with other predictors}
## Adding social predictors

data$c.educ4 <- scale(data$educ/4, scale=FALSE)
fit.7 <- glm (switch ~ c.dist100 + c.arsenic + c.educ4 + c.dist100:c.arsenic, family=binomial(link="logit"), data=data)
summary(fit.7)
```

Moreover, we add the interaction between the new variable and the previous predictors `dist` and `arsenic`. 
The interpretation of the new interaction terms represents the effect of education on the effects of the other predictors. Positive changes in education reduce distance’s negative association and increasing education increases arsenic’s positive association.
 
```{r logistic with other predictors and interactions}
## Adding further interactions (centring education variable)

fit.8 <- glm (switch ~ c.dist100 + c.arsenic + c.educ4 + c.dist100:c.arsenic +
                c.dist100:c.educ4 + c.arsenic:c.educ4, family=binomial(link="logit"), data=data)
summary(fit.8)
```

###  Binned Residuals

Outcome variable in the logistic regression is discrete, so the residuals of the model. The plot of the residuals defined as 
$$residual_i = y_i - E(y_i|X_i)=y_i-\text{logit}^{-1}(X_i\beta)$$
versus the fitted values is not useful.

```{r residuals}
## Residual Plot

pred.8 <- fit.8$fitted.values

plot(c(0,1), c(-1,1), xlab="Estimated  Pr (switching)", ylab="Observed - estimated", type="n", main="Residual plot", mgp=c(2,.5,0))
abline (0,0, col="gray", lwd=.5)
points (pred.8, data$switch-pred.8, pch=20, cex=.2)
```

A more readable plot can be made using the binned residuals defined as: 

*dividing the data into categories (bins) based on their fitted values, and then plotting the average residual versus the average fitted value for each bin* (Gelman, Hill 2007). 

The number of bins have to be chosen such that each bin is computed on *enough* points such that the resulting plot is not too noisy (high number of bins) but can highlight patterns in the residuals (hided if the number of bins is too low).  

Grey lines depict $\pm 2$ standard error bounds, so we expect that 95% of the binned residuals falls between these two lines. The standard error is defined as $\sqrt{p(1-p)/n}$, where $n$ is the number of data used to compute each average residual.
```{r binned residuals}
### Binned residual Plot 

 ## Defining binned residuals

binned.resids <- function (x, y, nclass=sqrt(length(x))){
  breaks.index <- floor(length(x)*(1:(nclass))/nclass)
  breaks <- c (-Inf, sort(x)[breaks.index], Inf)
  output <- NULL
  xbreaks <- NULL
  x.binned <- as.numeric (cut (x, breaks))
  for (i in 1:nclass){
    items <- (1:length(x))[x.binned==i]
    x.range <- range(x[items])
    xbar <- mean(x[items])
    ybar <- mean(y[items])
    n <- length(items)
    sdev <- sd(y[items])
    output <- rbind (output, c(xbar, ybar, n, x.range, 2*sdev/sqrt(n)))
  }
  colnames (output) <- c ("xbar", "ybar", "n", "x.lo", "x.hi", "2se")
  return (list (binned=output, xbreaks=xbreaks))
}

 ## Binned residuals vs. estimated probability of switching

br.8 <- binned.resids (pred.8, data$switch-pred.8, nclass = 100)$binned
plot(range(br.8[,1]), range(br.8[,2],br.8[,6],-br.8[,6]), xlab="Estimated  Pr (switching)", ylab="Average residual", type="n", main="Binned residual plot", mgp=c(2,.5,0))
abline (0,0, col="gray", lwd=.5)
lines (br.8[,1], br.8[,6], col="gray", lwd=.5)
lines (br.8[,1], -br.8[,6], col="gray", lwd=.5)
points (br.8[,1], br.8[,2], pch=19, cex=.5)
```

A pattern can be identified looking at the plot of the binned residuals with respect to the arsenic level. Indeed, there are large negative binned residuals for some households with wells with small values of arsenic concentration. These points correspond to people that are less likely to switch well with respect to what predicted by the model (almost $-20\%$). Moreover, the distribution of the residuals shows a slight patter: the model seems to underestimate the probabilities of switching for medium arsenic level values while overestimates for high arsenic concentrations. 

```{r binned residuals vs predictors}
## Plot of binned residuals vs. inputs of interest

  # distance

br.dist <- binned.resids(data$dist, data$switch-pred.8, nclass=40)$binned
plot(range(br.dist[,1]), range(br.dist[,2],br.dist[,6],-br.dist[,6]), xlab="Distance to nearest safe well", ylab="Average residual", type="n", main="Binned residual plot", mgp=c(2,.5,0))
abline (0,0, col="gray", lwd=.5)
lines (br.dist[,1], br.dist[,6], col="gray", lwd=.5)
lines (br.dist[,1], -br.dist[,6], col="gray", lwd=.5)
points (br.dist[,1], br.dist[,2], pch=19, cex=.5)

  # arsenic

br.arsenic <- binned.resids(data$arsenic, data$switch-pred.8, nclass=40)$binned
plot(range(0,br.arsenic[,1]), range(br.arsenic[,2],br.arsenic[,6],-br.arsenic[,6]), xlab="Arsenic level", ylab="Average residual", type="n", main="Binned residual plot", mgp=c(2,.5,0))
abline (0,0, col="gray", lwd=.5)
lines (br.arsenic[,1], br.arsenic[,6], col="gray", lwd=.5)
lines (br.arsenic[,1], -br.arsenic[,6], col="gray", lwd=.5)
points (br.arsenic[,1], br.arsenic[,2], pch=19, cex=.5)
```

**Extra** propose a possible solution to improve the model and fit your model. Compare the new binned residual plots with the previous ones.

### Error rate

Error rate is defined as the proportion of cases for which the fitted probabilities are above 0.5 and the outcome value is 0 or the fitted probabilities are below 0.5 and the outcome value is 1, that is when the prediction-guessing is wrong:
$$er = mean((y=0 \& E(y|X)>.5)|(y=1 \& E(y|X)<.5))$$
It should be a value between 0 (perfect prediction-guessing) and 0.5 (random prediction-guessing).

Usually we expect the error rate to be lower than the error rate of the null model, that is the only-intercept model. The estimated probability for the null model will be $p=\sum_i y_i/n$, that is the proportion of 1's in the data. The error rate for the null model is $min(p, 1-p)$, and it corresponds to assign 1 (or 0) to all the fitted values.
```{r error rate}
glm (switch ~ 1,family=binomial(link="logit"), data=data)$coefficients
invlogit(0.3029584); mean(data$switch)
error.rate <- mean((rep(1,length(data$switch))>0.5 & data$switch==0) | 
                     ((rep(1,length(data$switch))<0.5 & data$switch==1)))
error.rate
```
So if our model improved the prediction-guessing of a simple logistic regression, we expect the error rate of our model to be lower than the error rate for the null model. Our last fitted model, with an error rate of $0.38$, correctly predicts $62\%$ of the switching choices, only a $4\%$ of improvement if compared to the simply guessing that all the households will switch. 

```{r error rate model}
error.rate <- mean((pred.8>0.5 & data$switch==0) | (pred.8<0.5 & data$switch==1))
error.rate
```

Consider the error rate as the equivalent of the $R^2$ for the linear model, it can be a useful measure of the model fit but can't substitute a deeper evaluation of the model looking at coefficients significance and at the diagnostic plot.  

**Extra** justify the 'high' error rate we obtained with model `fit.8`.

**Extra** compute the error rate for your model and compare it with the previous one.


## Separation in logistic regression

Let simulate some data to fit a logistic regression with two predictors. The first is from a standard normal distribution and the second from Bernoulli with parameter 0.5.

```{r}
library("arm")

n <- 100
x1 <- rnorm (n)
x2 <- rbinom (n, 1, .5)
b0 <- 1
b1 <- 1.5
b2 <- 2
y <- rbinom (n, 1, arm::invlogit(b0+b1*x1+b2*x2))

#simple logistic regression
M1 <- glm (y ~ x1 + x2, family=binomial(link="logit"))
display (M1)
```

Now we plot the fitted logistic curve when the dichotomous predictor is equal to 0 and 1.
```{r}
invlogit <- function(x, b0, b1, b2, ind){
  1/(1+exp(-(b0+ b1*x + b2*ind)))
}

#logistic curve for the simple logistic model when x2=0
curve(invlogit(x, b0 = M1$coefficients[1],
               b1 = M1$coefficients[2], 
               b2 = M1$coefficients[3],
               ind = 0), 
      ylab ="invlogit", xlab ="x1",
      xlim=c(-5,5), ylim =c(0,1))

#logistic curve for the simple logistic model when x2=1
curve(invlogit(x, b0 = M1$coefficients[1],
               b1 = M1$coefficients[2], 
               b2 = M1$coefficients[3], ind = 1),
      ylab ="invlogit", xlab ="x1",lty =2, add =TRUE)
```

To create quasi complete separation we assign value 1 to all the ys when x2 = 1. 

```{r}
y <- ifelse (x2==1, 1, y)

M1 <- glm (y ~ x1 + x2, family=binomial(link="logit"))
display (M1)

## x2=0
curve(invlogit(x, b0 = M1$coefficients[1],
               b1 = M1$coefficients[2], 
               b2 = M1$coefficients[3], 
               ind = 0), 
      ylab ="invlogit", xlab ="x1",
      xlim=c(-15,5), ylim =c(0,1))

##x2=1
curve(invlogit(x, b0 = M1$coefficients[1],
               b1 = M1$coefficients[2], 
               b2 = M1$coefficients[3],
               ind = 1),  
      ylab ="invlogit", xlab ="x1",lty =2, add =TRUE)
```

Here we create separation setting y=1 whenever x1>0. Now the value of y can be exactly predicted by x1 and the best-fit logistic regression line is $y = logit^{−1}(\infty(x1))$, which has an infinite slope at x1=0.
```{r}
y <- ifelse (x1>0, 1, 0)

M1 <- glm (y ~ x1, family=binomial(link="logit"))
display (M1)

## 
curve(arm::invlogit(M1$coefficients[1]+M1$coefficients[2]*x), 
      ylab ="invlogit", xlab ="x1",
      xlim=c(-5,5), ylim =c(0,1))

```

