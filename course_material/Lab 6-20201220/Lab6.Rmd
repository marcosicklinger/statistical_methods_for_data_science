---
title: "Laboratory 6"
author: "G Di Credico, L Egidi, N Torelli"
date: "Fall 2020"
output:
  html_document:
    css: style.css
    toc: yes
  beamer_presentation:
    highlight: tango
  pdf_document:
    highlight: tango
    keep_tex: yes
    toc: yes
  slidy_presentation:
    fig.height: 3
    fig.width: 4
    highlight: tango
  ioslides_presentation:
    highlight: tango
  slide_level: 2
  include: null
header-includes:
- \usepackage{color}
- \usepackage{bm}
- \definecolor{Purple}{HTML}{911146}
- \definecolor{Orange}{HTML}{CF4A30}
- \setbeamercolor{alerted text}{fg=Orange}
- \setbeamercolor{frametitle}{bg=Purple}
graphics: yes
subtitle: Simulation, examples and exercises
fontsize: 10pt
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10))#, fig.path = 'figs/')
library(MASS)
```
```{r setup, include=FALSE}
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
```

# Spline functions

Splines are functions defined as piecewise polynomials of fixed degree. The points of connections of the polynomials are called knots. Different basis expansions can be used to define a spline function. 

We use a naive application on `Boston` dataset available in the `MASS` package to compare the use of polynomials and splines. The dataset collects information on housing values in suburbs of Boston and in particular we are going to model the continuous variable `medv` (median value of owner-occupied homes in $1000s) using a simple linear model including the continuous variable `lstat` (lower status of the population) as a predictor. 

```{r}
library(ggplot2)
data("Boston", package = "MASS")
ggplot(Boston, aes(lstat, medv) ) +
  geom_point() 
fit <- lm(medv ~ lstat, data = Boston)
par(mfrow=c(2,2))
plot(fit)

fit.poly <- lm(medv ~ lstat + I(lstat^2), data = Boston)
fit.poly2 <- lm(medv ~ poly(lstat, degree = 2, raw = TRUE), data = Boston)
summary(fit.poly2); summary(fit.poly)

fit.poly5 <- lm(medv ~ poly(lstat, 5), data = Boston)

ggplot(Boston, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = lm, formula = y ~ poly(x, 5, raw = TRUE))
```

```{r}
library(splines)
knots <- quantile(Boston$lstat)[2:4]
fit.spline <- lm (medv ~ bs(lstat, knots = knots), data = Boston)
summary(fit.spline)
fit.spline <- lm (medv ~ bs(lstat, df = 6), data = Boston)
summary(fit.spline)
ggplot(Boston, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = lm, formula = y ~ poly(x, 3, raw = TRUE))+
  stat_smooth(method = lm, formula = y ~ splines::bs(x, df = 6), col="red")+
  stat_smooth(method = lm, formula = y ~ splines::bs(x, df = 100), col="green")
```

The next plot shows a comparison of 3 spline functions with 1 knot on the median of the predictor and different spline degrees. 
```{r}
ggplot(Boston, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = lm, formula = y ~ splines::bs(x, knots = median(Boston$lstat)))+
  stat_smooth(method = lm, formula = y ~ splines::bs(x, knots = median(Boston$lstat),
                                                     degree=2),col="red")+
    stat_smooth(method = lm, formula = y ~ splines::bs(x, knots = median(Boston$lstat),
                                                     degree=1),col="green")

```

# Generalized additive models (GAM)

A generalized additive model (GAM) is a generalized linear model (GLM) in which the linear predictor is given by a user specified sum of smooth functions of the covariates plus a conventional parametric component of the linear predictor. Given $\mu_i=E[Y_i]$, a simple example is:

$$\log(\mu_i)= \beta_0+\sum_{j=1}^{p}s_j(x_{ij})+\epsilon_i, \ i=1,\ldots,n,$$
where the dependent variable $Y_{i} \sim \mathsf{Gamma}(\mu_i, \phi)$ and $s_j$ are smooth functions of covariates $x_j$. GAM models imply the following **likelihood penalization** 

$$ l(\boldsymbol{\beta}, \boldsymbol{s})-\lambda R(\boldsymbol{s}),$$

where $R(\boldsymbol{s})$ is a measure of roughness.

- if $\lambda \rightarrow \infty \Rightarrow$ **maximal smoothness**. The fitted curve $s(\cdot)$ is a straight line. The *effective number of parameters*  associated to the predictor $x_j$ is 1. No need for a smooth function.

- if $\lambda \rightarrow 0\Rightarrow$ **no smoothness**. No penalty is considered and the *effective number of parameters*  associated to the predictor $x_j$ is greater than 1. 


## Trees dataset


As an example, consider the simple dataset `trees`, where we have measurements of the girth, height and volume of timber in 31 felled black cherry trees. Note that girth is the diameter of the tree (in inches) measured at 4 ft 6 in above the ground. First of all, we use the `pairs` plot for checking possible correlations between the three variables.


```{r pairs, eval=TRUE, echo=FALSE}
library(mgcv)
panel.hist <- function(x, ...){
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "grey", ...)
}
pairs(trees, panel = panel.smooth, main = "trees data",diag.panel = panel.hist)
```

### The model 

In order to give a general idea about the GAM flexibility, we may start with one predictor. We could  fit a simple Generalized Linear Model (GLM), and a GAM model $Y_{i} \sim \mathsf{Gamma}(\mu_i, \phi)$, such that:

\begin{align*}
\log(\mu_i)= & \beta_0+\beta_1 \mathsf{Height}_i\\
\log(\mu_i)= & \beta_0+ s_1(\mathsf{Height}_i),
\end{align*}

for $i=1,\ldots,n$. The question we pose is whether the GLM structure is good enough for fitting these data. We could start by printing the output of the GAM model; then, we may plot the predicted values for both the models.

```{r glm.1 vs gam.1}
glm.1 <- glm(Volume ~ Height, family = Gamma(link=log), data=trees)
gam.1 <- gam(Volume ~ s(Height), family=Gamma(link=log), data=trees)

summary(gam.1)

plot(trees$Height, trees$Volume, xlab="Height", ylab="Fitted values")
points(trees$Height, glm.1$fitted.values, col="blue", bg=3, pch=19)
points(trees$Height, gam.1$fitted.values, col="red", bg=2, pch=19, cex=.5)
legend("topright", c("GLM","GAM"), pch=19, col=c("blue", "red"))
```
Diagnostic plot involve the representation of the smooth function and the partial residuals defined as:
$$\hat\epsilon^{part}_{ij}=\hat s_j(x_{ij})+\hat\epsilon_i^{P}$$
where $\hat\epsilon^{P}$ are the Pearson residuals of the model. Looking at this plot we are interested in noticing non linearity or wiggle behavior of the smooth function and if the partial residuals are evenly distributed around the function. 
```{r gam.1 residuals}
plot(gam.1,residuals=TRUE,pch=19)
```

By the joint analysis of the summary output---look at the value ```edf=1```---the prediction plots and the final plot displaying $s(\mathsf{Height})$ versus $\mathsf{Height}$, we may conclude that this simple single predictor framework may be well explained by a GLM approach.


### Adding another predictor
Consider now to add the predictor $\mathsf{Girth}$, and take the same model as before, $Y_{i}=\mathsf{Volume}_i\sim \mathsf{Gamma}(\mu_i, \phi)$, then:

$$\log(\mu_i)=s_1(\mathsf{Height}_i)+s_2(\mathsf{Girth}_i).$$
Analogously as before, we fit the GLM and the GAM model. We plot together the fitted values and we separately analyze the smooth terms $s_1(\mathsf{Height})$ and $s_2(\mathsf{Girth})$.

```{r second}
glm.2<-glm(Volume ~ Girth + Height, family = Gamma(link=log), data=trees)
gam.2 <- gam(Volume ~ s(Height) + s(Girth), family=Gamma(link=log), data=trees)
summary(gam.2)
```


```{r plot, echo=FALSE}
par(mfrow=c(1,2))
plot(trees$Height, trees$Volume, xlab="Height", ylab="Fitted values")
points(trees$Height, glm.2$fitted.values, col="blue", pch=22, bg="blue")
points(trees$Height, gam.2$fitted.values, col="red", pch=23, bg="red")
legend("topright", c("GLM","GAM"), pch=c(22,23), col=c("blue", "red"))

plot(trees$Girth, trees$Volume, xlab="Girth", ylab="Fitted values")
points(trees$Girth, glm.2$fitted.values, col="blue", pch=22, bg="blue")
points(trees$Girth, gam.2$fitted.values, col="red", pch=23, bg="red")
legend("topright", c("GLM","GAM"), pch=c(22,23), col=c("blue", "red"))


par(mfrow=c(1,2))
plot(gam.2, residuals =TRUE, pch =19)
```
Options to visualize the fitted surface are collected in the function ```vis.gam ``` which offers perspective and contour plot for a gam model.
```{r vis.gam, warning=FALSE}
par(mfrow=c(1,2))
vis.gam(gam.2,theta=-45,type = "response", color="terrain")
vis.gam(gam.2,theta=-45,type = "link", color="terrain")
par(mfrow=c(1,2))
vis.gam(gam.2,type = "response", color="terrain", plot.type = "contour")
vis.gam(gam.2,type = "link", color="terrain", plot.type = "contour")

```

The output reports an ```edf``` equals 2.422 for the $\mathsf{Girth}$ predictor, meaning that a straight line could be partially inappropriate for our purposes. As a further check, we may compute the AIC for the four proposed models:

```{r aic}
AIC(glm.1, gam.1, glm.2, gam.2)
```

The AIC for the single predictor models coincide. The AIC dramatically decreases when considering the second predictor, with the GAM model favorite over the GLM model.

An issue we should consider is the so called **optimal degree of smoothness**. With the argument ```sp``` in the ```gam``` function we can manually control the degree of smoothness for each smooth function included in the model. 

```{r gam.2 playing with basis dimension and penalization}
#unpenalized regression spline
gam.2.1 <- gam(Volume ~ s(Height, k=10, fx=TRUE)+
                 s(Girth, k=10, fx=TRUE),
               family=Gamma(link=log), data=trees)
gam.2.1$sp
summary(gam.2.1)
```


```{r gam.2 playing with basis dimension and penalization-graph, warning=FALSE}
par(mfrow=c(1,2))
plot(gam.2.1,residuals=TRUE,pch=19)
par(mfrow=c(1,1))
vis.gam(gam.2.1,theta=-45,type = "link", color="terrain")

```

However, usually $\lambda$ may be estimated by several methods, such as CV, AIC, BIC...By default, ```gam``` function estimates this quantity via AIC. Let's take a look explicitly at the AIC and how $\lambda$ is selected.

```{r lambda_aic}
#extract the values
sp <- gam.2$sp

tuning.scale<-c(1e-5,1e-4,1e-3,1e-2,1e-1,1e0,1e1,1e2,1e3,1e4,1e5) 
scale.exponent <- log10(tuning.scale) 
n.tuning <- length(tuning.scale) 

minus2loglik <- rep(NA,n.tuning) 
edf <- rep(NA,n.tuning)  
aic <- rep(NA,n.tuning)  


for (i in 1:n.tuning) {   
  gamobj <- gam(Volume ~ s(Height) + s(Girth), family=Gamma(link=log),
                data=trees, sp=tuning.scale[i]*sp) 
  minus2loglik[i] <- -2*logLik(gamobj) 
  edf[i]          <- sum(gamobj$edf)+1  
  aic[i]          <- AIC(gamobj)
}

par(mfrow=c(2,2)) 
plot(scale.exponent,minus2loglik,type="b",main="-2 log likelihood") 
plot(scale.exponent,edf,ylim=c(0,70),type="b",main="effective number of parameters")  
plot(scale.exponent,aic,type="b",main="AIC build-in function") 
plot(scale.exponent,minus2loglik+2*edf,type="b",main="AIC") 

# find the minimum
opt.tuning.scale <- tuning.scale[which.min(aic)]
opt.tuning.scale
opt.sp<-opt.tuning.scale*sp

# fitting the final model with the optimal level of smoothing

gam.2.opt <- gam(Volume ~ s(Height)+s(Girth), family=Gamma(link=log),data=trees,
                 sp=opt.sp)

AIC(gam.2, gam.2.opt)
```


**It's your turn!**

1- Simulate some Poisson data with the `gamSim` function contained in the `mgcv` package (Use the default arguments `eg=1` for the Gu and Wahba 4 univariate term example and `scale=0.1`).

2- Fit a Gam and print the results. Interpret the fit.

3- Produce some plots for each $x_j$ plotted against $s(x_j)$, and comment. Do you maybe drop some covariates?

4- Compute the AIC between your final gam and a glm.

```{r poisson_gam, echo =FALSE, results='hyde', eval=FALSE}
set.seed(123)
dat <- gamSim(1,n=200,dist="poisson",scale=.1)
b2  <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=poisson,data=dat)
summary(b2)
plot(b2,pages=1)
b3 <- gam(y~x0+s(x1)+s(x2)+x3,family=poisson,data=dat)
plot(b3, pages=1)
b2_glm <- glm(y~x0+x1+x2+x3,family=poisson,data=dat)
AIC(b2, b2_glm)
```


# Tree-based classification 

We consider the example of detection of email spam. Data are available from the UCI repository of machine learning databases (http://www.ics.uci.edu/~mlearn/MLRepository.html) and they collect informations about 4601 email items, 1813 classified as spam. 57 explanatory variables describe several characteristics of the data. Following the example in the Data Analyisis and Graphics using R, we will consider only 6 of them, mostly related to the frequency of specific words and characters in the email. In detail,

  - `crl.tot`: total length of words that are in capitals;
  - `dollar`: the frequency of the $ symbol, as a percentage of all characters;
  - `bang`: the frequency of the ! symbol, as a percentage of all characters;
  - `money`: frequency of the word “money”, as a percentage of all words;
  - `n000`: frequency of the character string “000”, as a percentage of all words; 
  - `make`: frequency of the word “make”, as a percentage of all words.

Using these 6 explanatory variables we want to build an decision tree model that is able to classify each email correctly as spam (`y` in the binary outcome variable `yesno`) and non-spam (`n` in the binary outcome variable `yesno`).

```{r tree load data}
library(reshape2)
library(rpart)
library(rpart.plot)
spam <- read.table("spambase.data", sep=",")
spam <- spam[,c(58,#"yesno"
                57,#"crl.tot"
                53,#"dollar" 
                52,#"bang"
                24,#"money" 
                23,#"n000"
                1  #"make"
                )]
typeof(spam)
colnames(spam) <- c("yesno","crl.tot","dollar","bang", "money","n000", "make")
spam$yesno <- factor(spam$yesno, levels = c(0, 1))
levels(spam$yesno) <- c("n","y")

par(mfrow=c(2,3))
for(i in 2:7){
  boxplot(spam[,i]~yesno, data=spam, ylab = colnames(spam)[i])
}
```


```{r logistic fails}
#summary(glm(yesno ~ crl.tot + dollar + bang + money + n000 + log(make+.5),
#            family=binomial, data=spam))
```

The model can be fitted using the function `rpart` in the library `rpart`. The option class in the method is selected by default when the outcome variable is of type factor. 
```{r tree}
library(rpart)

spam.rpart <- rpart(yesno ~ crl.tot + dollar + bang + money + n000 + make,
                    method="class", data=spam) 
plot(spam.rpart) # Draw tree
text(spam.rpart) # Add labeling
```

The summary of the fitted decision tree is visualized using the function `printcp`.
```{r}
printcp(spam.rpart)
```
The complexity parameter `CP` is a proxy for the number of splits. In order to avoid too complex trees, the reduction of lack-of-fit for each additional split is evaluated with an increasing cost. When the cost outweights the reduction, the algorithm stops.

Small complexity parameter `CP` leads to large tree and large `CP`leads to small tree.
Let's try to decrease the default `CP` value for our model.

```{r}
spam.rpart0001 <- rpart(yesno ~ crl.tot + dollar + bang + money + n000 + make,
                    method="class", data=spam, cp = 0) 
printcp(spam.rpart0001)
```

The relative error showed in the summary decreases at any additional split, so it is not useful to evaluate the predictive accuracy of the model, while the xerror (which stands for cross-validated relative error) reaches a minimum. Since the xerror is computed using 10-fold cross-validation procedure by default, the values slightly changes everytime we fit a new model. The relative error remains exactly the same.
```{r}
plotcp(spam.rpart0001)
```

Now that we have a very large (overfitted) tree, what is the best number of splits where to prune our tree? Changes in the xerrors are so small that  running the model again would probably lead to a different choice of number of splits if it is based on selecting the tree with the absolute minimum xerror. To reduce instabilty in the choice we can use the one-standard-deviation rule. The horizontal dashed line in the plot shows the minimum xerror + standard deviation. So choosing the smallest tree whose xerror is less or equal than this value will lead us to a more conservative choice if the interest is in choosing the optimal predictive tree, i.e., the predictive power will on average be slightly less than optimal.
```{r}

best.cp <- spam.rpart0001$cptable[which.min(spam.rpart0001$cptable[,"xerror"]),]
best.cp

sd.rule <- best.cp["xerror"]+best.cp["xstd"]
cptable.sd.rule <- spam.rpart0001$cptable[spam.rpart0001$cptable[,"xerror"]<=sd.rule,]
best.cp.sd <- cptable.sd.rule[which.min(cptable.sd.rule[,"nsplit"]),]
best.cp.sd

tree.pruned <- prune(spam.rpart0001, cp=best.cp[1])
rpart.plot(tree.pruned, extra=106, box.palette="GnBu",branch.lty=3, shadow.col="gray", nn=TRUE)

tree.pruned.sd <- prune(spam.rpart0001, cp=best.cp.sd[1])
printcp(tree.pruned.sd)
rpart.plot(tree.pruned.sd, extra=106, box.palette="GnBu",branch.lty=3, shadow.col="gray", nn=TRUE)

```
Absolute cross validation error for this last model is: $0.33756*0.39404=0.1330121$, thus the model achieved an error rate of 13.3\%. 

Trying with RandomForest..
```{r}
library(randomForest)
spam.rf <- randomForest(yesno ~ ., data=spam, importance=TRUE) 
print(spam.rf)
importance(spam.rf)
```

